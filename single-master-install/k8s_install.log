Last login: Sat Apr 11 18:12:57 2020 from 192.168.100.115
[root@k8s61 ~]# vim /etc/host
host.conf    hostname     hosts        hosts.allow  hosts.deny   
[root@k8s61 ~]# vim /etc/host
host.conf    hostname     hosts        hosts.allow  hosts.deny   
[root@k8s61 ~]# vim /etc/hosts
[root@k8s61 ~]# echo "192.168.122.50  openstack50" >> /etc/hosts
[root@k8s61 ~]# echo "192.168.122.60  kubernetes.haproxy.com" >> /etc/hosts
[root@k8s61 ~]# echo "192.168.122.61  k8s61" >> /etc/hosts
[root@k8s61 ~]# echo "192.168.122.62  k8s62" >> /etc/hosts
[root@k8s61 ~]# echo "192.168.122.63  k8s63" >> /etc/hosts
[root@k8s61 ~]# echo "192.168.122.64  k8s64" >> /etc/hosts
[root@k8s61 ~]# echo "192.168.122.65  k8s65" >> /etc/hosts
[root@k8s61 ~]# echo "192.168.122.71  ceph71" >> /etc/hosts
[root@k8s61 ~]# echo "192.168.122.72  ceph72" >> /etc/hosts
[root@k8s61 ~]# echo "192.168.122.73  ceph73" >> /etc/hosts
[root@k8s61 ~]# vim /etc/hosts
[root@k8s61 ~]# # 导入官网提供的repo
[root@k8s61 ~]# sudo yum install -y yum-utils device-mapper-persistent-data lvm2
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
base                                                                                                                                                        | 3.6 kB  00:00:00     
epel                                                                                                                                                        | 4.7 kB  00:00:00     
extras                                                                                                                                                      | 2.9 kB  00:00:00     
updates                                                                                                                                                     | 2.9 kB  00:00:00     
(1/2): epel/x86_64/updateinfo                                                                                                                               | 1.0 MB  00:00:15     
(2/2): epel/x86_64/primary_db                                                                                                                               | 6.8 MB  00:00:16     
Package yum-utils-1.1.31-52.el7.noarch already installed and latest version
Package device-mapper-persistent-data-0.8.5-1.el7.x86_64 already installed and latest version
Package 7:lvm2-2.02.185-2.el7_7.2.x86_64 already installed and latest version
Nothing to do
[root@k8s61 ~]# sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
Loaded plugins: fastestmirror
adding repo from: https://download.docker.com/linux/centos/docker-ce.repo
grabbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo
Could not fetch/save url https://download.docker.com/linux/centos/docker-ce.repo to file /etc/yum.repos.d/docker-ce.repo: [Errno 14] curl#7 - "Failed connect to download.docker.com:443; Operation now in progress"
[root@k8s61 ~]# 
[root@k8s61 ~]# # 安装docker及其相关组件
[root@k8s61 ~]# sudo yum install -y docker-ce docker-ce-cli containerd.io
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
No package docker-ce available.
No package docker-ce-cli available.
No package containerd.io available.
Error: Nothing to do
[root@k8s61 ~]# 
[root@k8s61 ~]# # 启动docker
[root@k8s61 ~]# sudo systemctl start docker
Failed to start docker.service: Unit not found.
[root@k8s61 ~]# sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
Loaded plugins: fastestmirror
adding repo from: https://download.docker.com/linux/centos/docker-ce.repo
grabbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo
Could not fetch/save url https://download.docker.com/linux/centos/docker-ce.repo to file /etc/yum.repos.d/docker-ce.repo: [Errno 14] curl#7 - "Failed connect to download.docker.com:443; Operation now in progress"
[root@k8s61 ~]# lsmod | grep br_netfilter
[root@k8s61 ~]# modprobe br_netfilter
[root@k8s61 ~]# cat <<EOF > /etc/sysctl.d/k8s.conf
> net.ipv4.tcp_keepalive_time = 600
> net.ipv4.tcp_keepalive_intvl = 30
> net.ipv4.tcp_keepalive_probes = 10
> net.ipv4.neigh.default.gc_stale_time = 120
> net.ipv4.conf.all.rp_filter = 0
> net.ipv4.conf.default.rp_filter = 0
> net.ipv4.conf.default.arp_announce = 2
> net.ipv4.conf.lo.arp_announce = 2
> net.ipv4.conf.all.arp_announce = 2
> net.ipv4.ip_forward = 1
> net.ipv4.tcp_max_tw_buckets = 5000
> net.ipv4.tcp_syncookies = 1
> net.ipv4.tcp_max_syn_backlog = 1024
> net.ipv4.tcp_synack_retries = 2
> net.bridge.bridge-nf-call-ip6tables = 1
> net.bridge.bridge-nf-call-iptables = 1
> net.netfilter.nf_conntrack_max = 2310720
> fs.inotify.max_user_watches=89100
> fs.may_detach_mounts = 1
> fs.file-max = 52706963
> fs.nr_open = 52706963
> net.bridge.bridge-nf-call-arptables = 1
> vm.swappiness = 0
> vm.overcommit_memory=1
> vm.panic_on_oom=0
> EOF
[root@k8s61 ~]# yum install ipvsadm ipset -y
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
Package ipset-7.1-1.el7.x86_64 already installed and latest version
Resolving Dependencies
--> Running transaction check
---> Package ipvsadm.x86_64 0:1.27-7.el7 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

=================================================================================================================================================================================== Package                                    Arch                                      Version                                        Repository                               Size
===================================================================================================================================================================================Installing:
 ipvsadm                                    x86_64                                    1.27-7.el7                                     base                                     45 k

Transaction Summary
===================================================================================================================================================================================Install  1 Package

Total download size: 45 k
Installed size: 75 k
Downloading packages:
ipvsadm-1.27-7.el7.x86_64.rpm                                                                                                                               |  45 kB  00:00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : ipvsadm-1.27-7.el7.x86_64                                                                                                                                       1/1 
  Verifying  : ipvsadm-1.27-7.el7.x86_64                                                                                                                                       1/1 

Installed:
  ipvsadm.x86_64 0:1.27-7.el7                                                                                                                                                      

Complete!
[root@k8s61 ~]# sysctl --system
* Applying /usr/lib/sysctl.d/00-system.conf ...
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
kernel.yama.ptrace_scope = 0
* Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq = 16
kernel.core_uses_pid = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.all.promote_secondaries = 1
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_probes = 10
net.ipv4.neigh.default.gc_stale_time = 120
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0
net.ipv4.conf.default.arp_announce = 2
net.ipv4.conf.lo.arp_announce = 2
net.ipv4.conf.all.arp_announce = 2
net.ipv4.ip_forward = 1
net.ipv4.tcp_max_tw_buckets = 5000
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 1024
net.ipv4.tcp_synack_retries = 2
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
fs.inotify.max_user_watches = 89100
fs.may_detach_mounts = 1
fs.file-max = 52706963
fs.nr_open = 52706963
net.bridge.bridge-nf-call-arptables = 1
vm.swappiness = 0
vm.overcommit_memory = 1
vm.panic_on_oom = 0
* Applying /etc/sysctl.conf ...
[root@k8s61 ~]# modprobe -- ip_vs
[root@k8s61 ~]# modprobe -- ip_vs_rr
[root@k8s61 ~]# modprobe -- ip_vs_wrr
[root@k8s61 ~]# modprobe -- ip_vs_sh
[root@k8s61 ~]# modprobe -- nf_conntrack_ipv4
[root@k8s61 ~]# lsmod | grep ip_vs
ip_vs_sh               12688  0 
ip_vs_wrr              12697  0 
ip_vs_rr               12600  0 
ip_vs                 145497  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr
nf_conntrack          139224  2 ip_vs,nf_conntrack_ipv4
libcrc32c              12644  3 xfs,ip_vs,nf_conntrack
[root@k8s61 ~]# cat <<EOF >> /etc/rc.local
> modprobe -- ip_vs
> modprobe -- ip_vs_rr
> modprobe -- ip_vs_wrr
> modprobe -- ip_vs_sh
> modprobe -- nf_conntrack_ipv4
> EOF
[root@k8s61 ~]# chmod +x /etc/rc.d/rc.local
[root@k8s61 ~]# cat >> /etc/sysctl.conf <<EOF
> net.bridge.bridge-nf-call-ip6tables = 1
> net.bridge.bridge-nf-call-iptables = 1
> EOF
[root@k8s61 ~]# 
[root@k8s61 ~]# yum install -y bridge-utils.x86_64
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
Resolving Dependencies
--> Running transaction check
---> Package bridge-utils.x86_64 0:1.5-9.el7 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

=================================================================================================================================================================================== Package                                        Arch                                     Version                                      Repository                              Size
===================================================================================================================================================================================Installing:
 bridge-utils                                   x86_64                                   1.5-9.el7                                    base                                    32 k

Transaction Summary
===================================================================================================================================================================================Install  1 Package

Total download size: 32 k
Installed size: 56 k
Downloading packages:
bridge-utils-1.5-9.el7.x86_64.rpm                                                                                                                           |  32 kB  00:00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : bridge-utils-1.5-9.el7.x86_64                                                                                                                                   1/1 
  Verifying  : bridge-utils-1.5-9.el7.x86_64                                                                                                                                   1/1 

Installed:
  bridge-utils.x86_64 0:1.5-9.el7                                                                                                                                                  

Complete!
[root@k8s61 ~]# modprobe bridge
[root@k8s61 ~]# modprobe br_netfilter
[root@k8s61 ~]# 
[root@k8s61 ~]# sysctl -p
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
[root@k8s61 ~]# rpm -qa | grep docker
[root@k8s61 ~]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
Loaded plugins: fastestmirror
adding repo from: http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
grabbing file http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo
repo saved to /etc/yum.repos.d/docker-ce.repo
[root@k8s61 ~]# yum list docker-ce --showduplicates | sort -r
Loading mirror speeds from cached hostfile
Loaded plugins: fastestmirror
docker-ce.x86_64            3:19.03.8-3.el7                     docker-ce-stable
docker-ce.x86_64            3:19.03.7-3.el7                     docker-ce-stable
docker-ce.x86_64            3:19.03.6-3.el7                     docker-ce-stable
docker-ce.x86_64            3:19.03.5-3.el7                     docker-ce-stable
docker-ce.x86_64            3:19.03.4-3.el7                     docker-ce-stable
docker-ce.x86_64            3:19.03.3-3.el7                     docker-ce-stable
docker-ce.x86_64            3:19.03.2-3.el7                     docker-ce-stable
docker-ce.x86_64            3:19.03.1-3.el7                     docker-ce-stable
docker-ce.x86_64            3:19.03.0-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.9-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.8-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.7-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.6-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.5-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.4-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.3-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.2-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.1-3.el7                     docker-ce-stable
docker-ce.x86_64            3:18.09.0-3.el7                     docker-ce-stable
docker-ce.x86_64            18.06.3.ce-3.el7                    docker-ce-stable
docker-ce.x86_64            18.06.2.ce-3.el7                    docker-ce-stable
docker-ce.x86_64            18.06.1.ce-3.el7                    docker-ce-stable
docker-ce.x86_64            18.06.0.ce-3.el7                    docker-ce-stable
docker-ce.x86_64            18.03.1.ce-1.el7.centos             docker-ce-stable
docker-ce.x86_64            18.03.0.ce-1.el7.centos             docker-ce-stable
docker-ce.x86_64            17.12.1.ce-1.el7.centos             docker-ce-stable
docker-ce.x86_64            17.12.0.ce-1.el7.centos             docker-ce-stable
docker-ce.x86_64            17.09.1.ce-1.el7.centos             docker-ce-stable
docker-ce.x86_64            17.09.0.ce-1.el7.centos             docker-ce-stable
docker-ce.x86_64            17.06.2.ce-1.el7.centos             docker-ce-stable
docker-ce.x86_64            17.06.1.ce-1.el7.centos             docker-ce-stable
docker-ce.x86_64            17.06.0.ce-1.el7.centos             docker-ce-stable
docker-ce.x86_64            17.03.3.ce-1.el7                    docker-ce-stable
docker-ce.x86_64            17.03.2.ce-1.el7.centos             docker-ce-stable
docker-ce.x86_64            17.03.1.ce-1.el7.centos             docker-ce-stable
docker-ce.x86_64            17.03.0.ce-1.el7.centos             docker-ce-stable
Available Packages
[root@k8s61 ~]# sudo yum install docker-ce docker-ce-cli containerd.io
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
Resolving Dependencies
--> Running transaction check
---> Package containerd.io.x86_64 0:1.2.13-3.1.el7 will be installed
--> Processing Dependency: container-selinux >= 2:2.74 for package: containerd.io-1.2.13-3.1.el7.x86_64
---> Package docker-ce.x86_64 3:19.03.8-3.el7 will be installed
--> Processing Dependency: libcgroup for package: 3:docker-ce-19.03.8-3.el7.x86_64
---> Package docker-ce-cli.x86_64 1:19.03.8-3.el7 will be installed
--> Running transaction check
---> Package container-selinux.noarch 2:2.107-3.el7 will be installed
--> Processing Dependency: policycoreutils-python for package: 2:container-selinux-2.107-3.el7.noarch
---> Package libcgroup.x86_64 0:0.41-21.el7 will be installed
--> Running transaction check
---> Package policycoreutils-python.x86_64 0:2.5-33.el7 will be installed
--> Processing Dependency: setools-libs >= 3.3.8-4 for package: policycoreutils-python-2.5-33.el7.x86_64
--> Processing Dependency: libsemanage-python >= 2.5-14 for package: policycoreutils-python-2.5-33.el7.x86_64
--> Processing Dependency: audit-libs-python >= 2.1.3-4 for package: policycoreutils-python-2.5-33.el7.x86_64
--> Processing Dependency: python-IPy for package: policycoreutils-python-2.5-33.el7.x86_64
--> Processing Dependency: libqpol.so.1(VERS_1.4)(64bit) for package: policycoreutils-python-2.5-33.el7.x86_64
--> Processing Dependency: libqpol.so.1(VERS_1.2)(64bit) for package: policycoreutils-python-2.5-33.el7.x86_64
--> Processing Dependency: libapol.so.4(VERS_4.0)(64bit) for package: policycoreutils-python-2.5-33.el7.x86_64
--> Processing Dependency: checkpolicy for package: policycoreutils-python-2.5-33.el7.x86_64
--> Processing Dependency: libqpol.so.1()(64bit) for package: policycoreutils-python-2.5-33.el7.x86_64
--> Processing Dependency: libapol.so.4()(64bit) for package: policycoreutils-python-2.5-33.el7.x86_64
--> Running transaction check
---> Package audit-libs-python.x86_64 0:2.8.5-4.el7 will be installed
---> Package checkpolicy.x86_64 0:2.5-8.el7 will be installed
---> Package libsemanage-python.x86_64 0:2.5-14.el7 will be installed
---> Package python-IPy.noarch 0:0.75-6.el7 will be installed
---> Package setools-libs.x86_64 0:3.3.8-4.el7 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

=================================================================================================================================================================================== Package                                           Arch                              Version                                     Repository                                   Size
===================================================================================================================================================================================Installing:
 containerd.io                                     x86_64                            1.2.13-3.1.el7                              docker-ce-stable                             23 M
 docker-ce                                         x86_64                            3:19.03.8-3.el7                             docker-ce-stable                             25 M
 docker-ce-cli                                     x86_64                            1:19.03.8-3.el7                             docker-ce-stable                             40 M
Installing for dependencies:
 audit-libs-python                                 x86_64                            2.8.5-4.el7                                 base                                         76 k
 checkpolicy                                       x86_64                            2.5-8.el7                                   base                                        295 k
 container-selinux                                 noarch                            2:2.107-3.el7                               extras                                       39 k
 libcgroup                                         x86_64                            0.41-21.el7                                 base                                         66 k
 libsemanage-python                                x86_64                            2.5-14.el7                                  base                                        113 k
 policycoreutils-python                            x86_64                            2.5-33.el7                                  base                                        457 k
 python-IPy                                        noarch                            0.75-6.el7                                  base                                         32 k
 setools-libs                                      x86_64                            3.3.8-4.el7                                 base                                        620 k

Transaction Summary
===================================================================================================================================================================================Install  3 Packages (+8 Dependent packages)

Total download size: 89 M
Installed size: 369 M
Is this ok [y/d/N]: y
Downloading packages:
(1/11): checkpolicy-2.5-8.el7.x86_64.rpm                                                                                                                    | 295 kB  00:00:00     
(2/11): audit-libs-python-2.8.5-4.el7.x86_64.rpm                                                                                                            |  76 kB  00:00:00     
(3/11): container-selinux-2.107-3.el7.noarch.rpm                                                                                                            |  39 kB  00:00:00     
warning: /var/cache/yum/x86_64/7/docker-ce-stable/packages/docker-ce-19.03.8-3.el7.x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID 621e9f35: NOKEY5 MB/s |  17 MB  00:00:04 ETA 
Public key for docker-ce-19.03.8-3.el7.x86_64.rpm is not installed
(4/11): docker-ce-19.03.8-3.el7.x86_64.rpm                                                                                                                  |  25 MB  00:00:01     
(5/11): libcgroup-0.41-21.el7.x86_64.rpm                                                                                                                    |  66 kB  00:00:00     
(6/11): libsemanage-python-2.5-14.el7.x86_64.rpm                                                                                                            | 113 kB  00:00:00     
(7/11): policycoreutils-python-2.5-33.el7.x86_64.rpm                                                                                                        | 457 kB  00:00:00     
(8/11): python-IPy-0.75-6.el7.noarch.rpm                                                                                                                    |  32 kB  00:00:00     
(9/11): setools-libs-3.3.8-4.el7.x86_64.rpm                                                                                                                 | 620 kB  00:00:00     
(10/11): docker-ce-cli-19.03.8-3.el7.x86_64.rpm                                                                                                             |  40 MB  00:00:01     
(11/11): containerd.io-1.2.13-3.1.el7.x86_64.rpm                                                                                                            |  23 MB  00:00:02     
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Total                                                                                                                                               30 MB/s |  89 MB  00:00:02     
Retrieving key from https://mirrors.aliyun.com/docker-ce/linux/centos/gpg
Importing GPG key 0x621E9F35:
 Userid     : "Docker Release (CE rpm) <docker@docker.com>"
 Fingerprint: 060a 61c5 1b55 8a7f 742b 77aa c52f eb6b 621e 9f35
 From       : https://mirrors.aliyun.com/docker-ce/linux/centos/gpg
Is this ok [y/N]: y
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : libcgroup-0.41-21.el7.x86_64                                                                                                                                   1/11 
  Installing : setools-libs-3.3.8-4.el7.x86_64                                                                                                                                2/11 
  Installing : audit-libs-python-2.8.5-4.el7.x86_64                                                                                                                           3/11 
  Installing : python-IPy-0.75-6.el7.noarch                                                                                                                                   4/11 
  Installing : 1:docker-ce-cli-19.03.8-3.el7.x86_64                                                                                                                           5/11 
  Installing : libsemanage-python-2.5-14.el7.x86_64                                                                                                                           6/11 
  Installing : checkpolicy-2.5-8.el7.x86_64                                                                                                                                   7/11 
  Installing : policycoreutils-python-2.5-33.el7.x86_64                                                                                                                       8/11 
  Installing : 2:container-selinux-2.107-3.el7.noarch                                                                                                                         9/11 
setsebool:  SELinux is disabled.
  Installing : containerd.io-1.2.13-3.1.el7.x86_64                                                                                                                           10/11 
  Installing : 3:docker-ce-19.03.8-3.el7.x86_64                                                                                                                              11/11 
  Verifying  : checkpolicy-2.5-8.el7.x86_64                                                                                                                                   1/11 
  Verifying  : policycoreutils-python-2.5-33.el7.x86_64                                                                                                                       2/11 
  Verifying  : libsemanage-python-2.5-14.el7.x86_64                                                                                                                           3/11 
  Verifying  : 1:docker-ce-cli-19.03.8-3.el7.x86_64                                                                                                                           4/11 
  Verifying  : 2:container-selinux-2.107-3.el7.noarch                                                                                                                         5/11 
  Verifying  : python-IPy-0.75-6.el7.noarch                                                                                                                                   6/11 
  Verifying  : containerd.io-1.2.13-3.1.el7.x86_64                                                                                                                            7/11 
  Verifying  : 3:docker-ce-19.03.8-3.el7.x86_64                                                                                                                               8/11 
  Verifying  : audit-libs-python-2.8.5-4.el7.x86_64                                                                                                                           9/11 
  Verifying  : setools-libs-3.3.8-4.el7.x86_64                                                                                                                               10/11 
  Verifying  : libcgroup-0.41-21.el7.x86_64                                                                                                                                  11/11 

Installed:
  containerd.io.x86_64 0:1.2.13-3.1.el7                        docker-ce.x86_64 3:19.03.8-3.el7                        docker-ce-cli.x86_64 1:19.03.8-3.el7                       

Dependency Installed:
  audit-libs-python.x86_64 0:2.8.5-4.el7      checkpolicy.x86_64 0:2.5-8.el7                  container-selinux.noarch 2:2.107-3.el7      libcgroup.x86_64 0:0.41-21.el7        
  libsemanage-python.x86_64 0:2.5-14.el7      policycoreutils-python.x86_64 0:2.5-33.el7      python-IPy.noarch 0:0.75-6.el7              setools-libs.x86_64 0:3.3.8-4.el7     

Complete!
[root@k8s61 ~]# cat > /etc/docker/daemon.json <<EOF
> {
>   "exec-opts": ["native.cgroupdriver=systemd"],
>   "log-driver": "json-file",
>   "log-opts": {
>     "max-size": "100m"
>   },
>   "storage-driver": "overlay2",
>   "storage-opts": [
>     "overlay2.override_kernel_check=true"
>   ]
> }
> EOF
-bash: /etc/docker/daemon.json: No such file or directory
[root@k8s61 ~]# mkdir /etc/docker
[root@k8s61 ~]# cat > /etc/docker/daemon.json <<EOF
> {
>   "exec-opts": ["native.cgroupdriver=systemd"],
>   "log-driver": "json-file",
>   "log-opts": {
>     "max-size": "100m"
>   },
>   "storage-driver": "overlay2",
>   "storage-opts": [
>     "overlay2.override_kernel_check=true"
>   ]
> }
> EOF
[root@k8s61 ~]# mkdir -p /etc/systemd/system/docker.service.d
[root@k8s61 ~]# systemctl daemon-reload
[root@k8s61 ~]# systemctl restart docker
[root@k8s61 ~]# yum install -y yum-utils device-mapper-persistent-data lvm2
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
Package yum-utils-1.1.31-52.el7.noarch already installed and latest version
Package device-mapper-persistent-data-0.8.5-1.el7.x86_64 already installed and latest version
Package 7:lvm2-2.02.185-2.el7_7.2.x86_64 already installed and latest version
Nothing to do
[root@k8s61 ~]# lsmod | grep br_netfilter
br_netfilter           22256  0 
bridge                151336  1 br_netfilter
[root@k8s61 ~]# yum install ipvsadm ipset -y
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
Package ipvsadm-1.27-7.el7.x86_64 already installed and latest version
Package ipset-7.1-1.el7.x86_64 already installed and latest version
Nothing to do
[root@k8s61 ~]# sysctl --system
* Applying /usr/lib/sysctl.d/00-system.conf ...
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
kernel.yama.ptrace_scope = 0
* Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq = 16
kernel.core_uses_pid = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.all.promote_secondaries = 1
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/99-sysctl.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
* Applying /etc/sysctl.d/k8s.conf ...
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_probes = 10
net.ipv4.neigh.default.gc_stale_time = 120
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0
net.ipv4.conf.default.arp_announce = 2
net.ipv4.conf.lo.arp_announce = 2
net.ipv4.conf.all.arp_announce = 2
net.ipv4.ip_forward = 1
net.ipv4.tcp_max_tw_buckets = 5000
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 1024
net.ipv4.tcp_synack_retries = 2
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.netfilter.nf_conntrack_max = 2310720
fs.inotify.max_user_watches = 89100
fs.may_detach_mounts = 1
fs.file-max = 52706963
fs.nr_open = 52706963
net.bridge.bridge-nf-call-arptables = 1
vm.swappiness = 0
vm.overcommit_memory = 1
vm.panic_on_oom = 0
* Applying /etc/sysctl.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
[root@k8s61 ~]# docker version
Client: Docker Engine - Community
 Version:           19.03.8
 API version:       1.40
 Go version:        go1.12.17
 Git commit:        afacb8b
 Built:             Wed Mar 11 01:27:04 2020
 OS/Arch:           linux/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          19.03.8
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.12.17
  Git commit:       afacb8b
  Built:            Wed Mar 11 01:25:42 2020
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.2.13
  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429
 runc:
  Version:          1.0.0-rc10
  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd
 docker-init:
  Version:          0.18.0
  GitCommit:        fec3683
[root@k8s61 ~]# systemctl enable docker
Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.
[root@k8s61 ~]# systemctl start docker
[root@k8s61 ~]# systemctl status docker
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
   Active: active (running) since Sat 2020-04-11 23:41:10 CST; 4min 13s ago
     Docs: https://docs.docker.com
 Main PID: 14373 (dockerd)
   CGroup: /system.slice/docker.service
           └─14373 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock

Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.213562708+08:00" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.213596547+08:00" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/... module=grpcApr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.213612268+08:00" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.642287573+08:00" level=info msg="Loading containers: start."
Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.954482141+08:00" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/... IP address"Apr 11 23:41:10 k8s61 dockerd[14373]: time="2020-04-11T23:41:10.091929787+08:00" level=info msg="Loading containers: done."
Apr 11 23:41:10 k8s61 dockerd[14373]: time="2020-04-11T23:41:10.160328980+08:00" level=info msg="Docker daemon" commit=afacb8b graphdriver(s)=overlay2 version=19.03.8
Apr 11 23:41:10 k8s61 dockerd[14373]: time="2020-04-11T23:41:10.160454460+08:00" level=info msg="Daemon has completed initialization"
Apr 11 23:41:10 k8s61 dockerd[14373]: time="2020-04-11T23:41:10.314089124+08:00" level=info msg="API listen on /var/run/docker.sock"
Apr 11 23:41:10 k8s61 systemd[1]: Started Docker Application Container Engine.
Hint: Some lines were ellipsized, use -l to show in full.
[root@k8s61 ~]# systemctl status docker -l
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
   Active: active (running) since Sat 2020-04-11 23:41:10 CST; 4min 17s ago
     Docs: https://docs.docker.com
 Main PID: 14373 (dockerd)
   CGroup: /system.slice/docker.service
           └─14373 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock

Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.213562708+08:00" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.213596547+08:00" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock 0  <nil>}] <nil>}" module=grpc
Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.213612268+08:00" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.642287573+08:00" level=info msg="Loading containers: start."
Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.954482141+08:00" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Apr 11 23:41:10 k8s61 dockerd[14373]: time="2020-04-11T23:41:10.091929787+08:00" level=info msg="Loading containers: done."
Apr 11 23:41:10 k8s61 dockerd[14373]: time="2020-04-11T23:41:10.160328980+08:00" level=info msg="Docker daemon" commit=afacb8b graphdriver(s)=overlay2 version=19.03.8
Apr 11 23:41:10 k8s61 dockerd[14373]: time="2020-04-11T23:41:10.160454460+08:00" level=info msg="Daemon has completed initialization"
Apr 11 23:41:10 k8s61 dockerd[14373]: time="2020-04-11T23:41:10.314089124+08:00" level=info msg="API listen on /var/run/docker.sock"
Apr 11 23:41:10 k8s61 systemd[1]: Started Docker Application Container Engine.
[root@k8s61 ~]# docker info
Client:
 Debug Mode: false

Server:
 Containers: 0
  Running: 0
  Paused: 0
  Stopped: 0
 Images: 0
 Server Version: 19.03.8
 Storage Driver: overlay2
  Backing Filesystem: <unknown>
  Supports d_type: true
  Native Overlay Diff: true
 Logging Driver: json-file
 Cgroup Driver: systemd
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
 Swarm: inactive
 Runtimes: runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: 7ad184331fa3e55e52b890ea95e65ba581ae3429
 runc version: dc9208a3303feef5b3839f4323d9beb36df0a9dd
 init version: fec3683
 Security Options:
  seccomp
   Profile: default
 Kernel Version: 3.10.0-1062.18.1.el7.x86_64
 Operating System: CentOS Linux 7 (Core)
 OSType: linux
 Architecture: x86_64
 CPUs: 4
 Total Memory: 7.795GiB
 Name: k8s61
 ID: 5OGC:OJT2:AYI3:KDHK:DQHT:O3LG:AKPI:ZNJN:KNW2:6ZN6:2HBV:RVZ6
 Docker Root Dir: /var/lib/docker
 Debug Mode: false
 Registry: https://index.docker.io/v1/
 Labels:
 Experimental: false
 Insecure Registries:
  127.0.0.0/8
 Live Restore Enabled: false

[root@k8s61 ~]# systemctl status docker -l
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
   Active: active (running) since Sat 2020-04-11 23:41:10 CST; 6min ago
     Docs: https://docs.docker.com
 Main PID: 14373 (dockerd)
   CGroup: /system.slice/docker.service
           └─14373 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock

Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.213562708+08:00" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.213596547+08:00" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock 0  <nil>}] <nil>}" module=grpc
Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.213612268+08:00" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.642287573+08:00" level=info msg="Loading containers: start."
Apr 11 23:41:09 k8s61 dockerd[14373]: time="2020-04-11T23:41:09.954482141+08:00" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Apr 11 23:41:10 k8s61 dockerd[14373]: time="2020-04-11T23:41:10.091929787+08:00" level=info msg="Loading containers: done."
Apr 11 23:41:10 k8s61 dockerd[14373]: time="2020-04-11T23:41:10.160328980+08:00" level=info msg="Docker daemon" commit=afacb8b graphdriver(s)=overlay2 version=19.03.8
Apr 11 23:41:10 k8s61 dockerd[14373]: time="2020-04-11T23:41:10.160454460+08:00" level=info msg="Daemon has completed initialization"
Apr 11 23:41:10 k8s61 dockerd[14373]: time="2020-04-11T23:41:10.314089124+08:00" level=info msg="API listen on /var/run/docker.sock"
Apr 11 23:41:10 k8s61 systemd[1]: Started Docker Application Container Engine.
[root@k8s61 ~]# init 0
Last login: Sat Apr 11 23:29:40 2020 from 192.168.100.115
[root@k8s61 ~]# uuidgen eth0 >> /etc/sysconfig/network-scripts/ifcfg-eth0
[root@k8s61 ~]# uuidgen enp2s0 >> /etc/sysconfig/network-scripts/ifcfg-enp2s0
[root@k8s61 ~]# vim /etc/hostname 
[root@k8s61 ~]# vim /etc/hosts
[root@k8s61 ~]# vim /etc/sysconfig/network-scripts/ifcfg-eth0
[root@k8s61 ~]# vim /etc/sysconfig/network-scripts/ifcfg-enp2s0 
[root@k8s61 ~]# init 6
Last login: Sat Apr 11 23:29:40 2020 from 192.168.100.115
[root@k8s61 ~]# vim /etc/hosts
[root@k8s61 ~]# vim /etc/hostname 
[root@k8s61 ~]# uuidgen eth0 >> /etc/sysconfig/network-scripts/ifcfg-eth0
[root@k8s61 ~]# uuidgen enp2s0 >> /etc/sysconfig/network-scripts/ifcfg-enp2s0
[root@k8s61 ~]# vim /etc/sysconfig/network-scripts/ifcfg-eth0
[root@k8s61 ~]# vim /etc/sysconfig/network-scripts/ifcfg-enp2s0 
[root@k8s61 ~]# init 6
Last login: Sat Apr 11 23:29:40 2020 from 192.168.100.115
[root@k8s61 ~]# # 新建一个yum源
[root@k8s61 ~]# cat >> /etc/yum.repos.d/kubernetes.repo <<EOF
> [kubernetes]
> name=Kubernetes
> baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
> enabled=1
> gpgcheck=1
> repo_gpgcheck=1
> gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
> exclude=kube*
> EOF
[root@k8s61 ~]# 
[root@k8s61 ~]# yum clean all
Loaded plugins: fastestmirror
Cleaning repos: base docker-ce-stable epel extras kubernetes updates
Cleaning up list of fastest mirrors
[root@k8s61 ~]# 
[root@k8s61 ~]# yum repolist -y
Loaded plugins: fastestmirror
Determining fastest mirrors
base                                                                                                                                                        | 3.6 kB  00:00:00     
docker-ce-stable                                                                                                                                            | 3.5 kB  00:00:00     
epel                                                                                                                                                        | 4.7 kB  00:00:00     
extras                                                                                                                                                      | 2.9 kB  00:00:00     
kubernetes/signature                                                                                                                                        |  454 B  00:00:00     
Retrieving key from https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
Importing GPG key 0xA7317B0F:
 Userid     : "Google Cloud Packages Automatic Signing Key <gc-team@google.com>"
 Fingerprint: d0bc 747f d8ca f711 7500 d6fa 3746 c208 a731 7b0f
 From       : https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
Retrieving key from https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
kubernetes/signature                                                                                                                                        | 1.4 kB  00:00:00 !!! 
updates                                                                                                                                                     | 2.9 kB  00:00:00     
(1/10): docker-ce-stable/x86_64/primary_db                                                                                                                  |  41 kB  00:00:00     
(2/10): docker-ce-stable/x86_64/updateinfo                                                                                                                  |   55 B  00:00:00     
(3/10): base/7/x86_64/group_gz                                                                                                                              | 165 kB  00:00:00     
(4/10): base/7/x86_64/primary_db                                                                                                                            | 6.0 MB  00:00:00     
(5/10): epel/x86_64/group_gz                                                                                                                                |  95 kB  00:00:15     
(6/10): extras/7/x86_64/primary_db                                                                                                                          | 165 kB  00:00:00     
(7/10): kubernetes/primary                                                                                                                                  |  66 kB  00:00:00     
(8/10): epel/x86_64/updateinfo                                                                                                                              | 1.0 MB  00:00:15     
(9/10): epel/x86_64/primary_db                                                                                                                              | 6.8 MB  00:00:00     
(10/10): updates/7/x86_64/primary_db                                                                                                                        | 7.6 MB  00:00:00     
kubernetes                                                                                                                                                                 484/484
repo id                                                                    repo name                                                                                         statusbase/7/x86_64                                                              CentOS-7 - Base                                                                                   10,097docker-ce-stable/x86_64                                                    Docker CE Stable - x86_64                                                                             70epel/x86_64                                                                Extra Packages for Enterprise Linux 7 - x86_64                                                    13,234extras/7/x86_64                                                            CentOS-7 - Extras                                                                                    341kubernetes                                                                 Kubernetes                                                                                         8+476updates/7/x86_64                                                           CentOS-7 - Updates                                                                                 1,787repolist: 25,537
[root@k8s61 ~]# 
[root@k8s61 ~]# yum install kubelet kubeadm kubectl --disableexcludes=kubernetes -y
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
Resolving Dependencies
--> Running transaction check
---> Package kubeadm.x86_64 0:1.18.1-0 will be installed
--> Processing Dependency: kubernetes-cni >= 0.7.5 for package: kubeadm-1.18.1-0.x86_64
--> Processing Dependency: cri-tools >= 1.13.0 for package: kubeadm-1.18.1-0.x86_64
---> Package kubectl.x86_64 0:1.18.1-0 will be installed
---> Package kubelet.x86_64 0:1.18.1-0 will be installed
--> Processing Dependency: socat for package: kubelet-1.18.1-0.x86_64
--> Processing Dependency: conntrack for package: kubelet-1.18.1-0.x86_64
--> Running transaction check
---> Package conntrack-tools.x86_64 0:1.4.4-5.el7_7.2 will be installed
--> Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.1)(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64
--> Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.0)(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64
--> Processing Dependency: libnetfilter_cthelper.so.0(LIBNETFILTER_CTHELPER_1.0)(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64
--> Processing Dependency: libnetfilter_queue.so.1()(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64
--> Processing Dependency: libnetfilter_cttimeout.so.1()(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64
--> Processing Dependency: libnetfilter_cthelper.so.0()(64bit) for package: conntrack-tools-1.4.4-5.el7_7.2.x86_64
---> Package cri-tools.x86_64 0:1.13.0-0 will be installed
---> Package kubernetes-cni.x86_64 0:0.7.5-0 will be installed
---> Package socat.x86_64 0:1.7.3.2-2.el7 will be installed
--> Running transaction check
---> Package libnetfilter_cthelper.x86_64 0:1.0.0-10.el7_7.1 will be installed
---> Package libnetfilter_cttimeout.x86_64 0:1.0.0-6.el7_7.1 will be installed
---> Package libnetfilter_queue.x86_64 0:1.0.2-2.el7_2 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

=================================================================================================================================================================================== Package                                            Arch                               Version                                        Repository                              Size
===================================================================================================================================================================================Installing:
 kubeadm                                            x86_64                             1.18.1-0                                       kubernetes                             8.8 M
 kubectl                                            x86_64                             1.18.1-0                                       kubernetes                             9.5 M
 kubelet                                            x86_64                             1.18.1-0                                       kubernetes                              21 M
Installing for dependencies:
 conntrack-tools                                    x86_64                             1.4.4-5.el7_7.2                                updates                                187 k
 cri-tools                                          x86_64                             1.13.0-0                                       kubernetes                             5.1 M
 kubernetes-cni                                     x86_64                             0.7.5-0                                        kubernetes                              10 M
 libnetfilter_cthelper                              x86_64                             1.0.0-10.el7_7.1                               updates                                 18 k
 libnetfilter_cttimeout                             x86_64                             1.0.0-6.el7_7.1                                updates                                 18 k
 libnetfilter_queue                                 x86_64                             1.0.2-2.el7_2                                  base                                    23 k
 socat                                              x86_64                             1.7.3.2-2.el7                                  base                                   290 k

Transaction Summary
===================================================================================================================================================================================Install  3 Packages (+7 Dependent packages)

Total download size: 55 M
Installed size: 246 M
Downloading packages:
(1/10): conntrack-tools-1.4.4-5.el7_7.2.x86_64.rpm                                                                                                          | 187 kB  00:00:00     
warning: /var/cache/yum/x86_64/7/kubernetes/packages/a86b51d48af8df740035f8bc4c0c30d994d5c8ef03388c21061372d5c5b2859d-kubeadm-1.18.1-0.x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID 3e1ba8d5: NOKEY
Public key for a86b51d48af8df740035f8bc4c0c30d994d5c8ef03388c21061372d5c5b2859d-kubeadm-1.18.1-0.x86_64.rpm is not installed
(2/10): a86b51d48af8df740035f8bc4c0c30d994d5c8ef03388c21061372d5c5b2859d-kubeadm-1.18.1-0.x86_64.rpm                                                        | 8.8 MB  00:00:00     
(3/10): 14bfe6e75a9efc8eca3f638eb22c7e2ce759c67f95b43b16fae4ebabde1549f3-cri-tools-1.13.0-0.x86_64.rpm                                                      | 5.1 MB  00:00:00     
(4/10): 9b65a188779e61866501eb4e8a07f38494d40af1454ba9232f98fd4ced4ba935-kubectl-1.18.1-0.x86_64.rpm                                                        | 9.5 MB  00:00:00     
(5/10): libnetfilter_cttimeout-1.0.0-6.el7_7.1.x86_64.rpm                                                                                                   |  18 kB  00:00:00     
(6/10): socat-1.7.3.2-2.el7.x86_64.rpm                                                                                                                      | 290 kB  00:00:00     
(7/10): libnetfilter_cthelper-1.0.0-10.el7_7.1.x86_64.rpm                                                                                                   |  18 kB  00:00:00     
(8/10): libnetfilter_queue-1.0.2-2.el7_2.x86_64.rpm                                                                                                         |  23 kB  00:00:00     
(9/10): 548a0dcd865c16a50980420ddfa5fbccb8b59621179798e6dc905c9bf8af3b34-kubernetes-cni-0.7.5-0.x86_64.rpm                                                  |  10 MB  00:00:00     
(10/10): 39b64bb11c6c123dd502af7d970cee95606dbf7fd62905de0412bdac5e875843-kubelet-1.18.1-0.x86_64.rpm                                                       |  21 MB  00:00:01     
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Total                                                                                                                                               21 MB/s |  55 MB  00:00:02     
Retrieving key from https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
Importing GPG key 0xA7317B0F:
 Userid     : "Google Cloud Packages Automatic Signing Key <gc-team@google.com>"
 Fingerprint: d0bc 747f d8ca f711 7500 d6fa 3746 c208 a731 7b0f
 From       : https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
Retrieving key from https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
Importing GPG key 0x3E1BA8D5:
 Userid     : "Google Cloud Packages RPM Signing Key <gc-team@google.com>"
 Fingerprint: 3749 e1ba 95a8 6ce0 5454 6ed2 f09c 394c 3e1b a8d5
 From       : https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : libnetfilter_cttimeout-1.0.0-6.el7_7.1.x86_64                                                                                                                  1/10 
  Installing : kubectl-1.18.1-0.x86_64                                                                                                                                        2/10 
  Installing : socat-1.7.3.2-2.el7.x86_64                                                                                                                                     3/10 
  Installing : cri-tools-1.13.0-0.x86_64                                                                                                                                      4/10 
  Installing : libnetfilter_queue-1.0.2-2.el7_2.x86_64                                                                                                                        5/10 
  Installing : libnetfilter_cthelper-1.0.0-10.el7_7.1.x86_64                                                                                                                  6/10 
  Installing : conntrack-tools-1.4.4-5.el7_7.2.x86_64                                                                                                                         7/10 
  Installing : kubernetes-cni-0.7.5-0.x86_64                                                                                                                                  8/10 
  Installing : kubelet-1.18.1-0.x86_64                                                                                                                                        9/10 
  Installing : kubeadm-1.18.1-0.x86_64                                                                                                                                       10/10 
  Verifying  : libnetfilter_cthelper-1.0.0-10.el7_7.1.x86_64                                                                                                                  1/10 
  Verifying  : kubeadm-1.18.1-0.x86_64                                                                                                                                        2/10 
  Verifying  : libnetfilter_queue-1.0.2-2.el7_2.x86_64                                                                                                                        3/10 
  Verifying  : kubelet-1.18.1-0.x86_64                                                                                                                                        4/10 
  Verifying  : cri-tools-1.13.0-0.x86_64                                                                                                                                      5/10 
  Verifying  : kubernetes-cni-0.7.5-0.x86_64                                                                                                                                  6/10 
  Verifying  : socat-1.7.3.2-2.el7.x86_64                                                                                                                                     7/10 
  Verifying  : kubectl-1.18.1-0.x86_64                                                                                                                                        8/10 
  Verifying  : libnetfilter_cttimeout-1.0.0-6.el7_7.1.x86_64                                                                                                                  9/10 
  Verifying  : conntrack-tools-1.4.4-5.el7_7.2.x86_64                                                                                                                        10/10 

Installed:
  kubeadm.x86_64 0:1.18.1-0                                  kubectl.x86_64 0:1.18.1-0                                  kubelet.x86_64 0:1.18.1-0                                 

Dependency Installed:
  conntrack-tools.x86_64 0:1.4.4-5.el7_7.2          cri-tools.x86_64 0:1.13.0-0                 kubernetes-cni.x86_64 0:0.7.5-0   libnetfilter_cthelper.x86_64 0:1.0.0-10.el7_7.1  
  libnetfilter_cttimeout.x86_64 0:1.0.0-6.el7_7.1   libnetfilter_queue.x86_64 0:1.0.2-2.el7_2   socat.x86_64 0:1.7.3.2-2.el7     

Complete!
[root@k8s61 ~]# # 启动kubelet并设置开机启动
[root@k8s61 ~]# systemctl enable kubelet
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.
[root@k8s61 ~]# vim /etc/sysconfig/kubelet
[root@k8s61 ~]# init 6
Last login: Sat Apr 11 23:54:16 2020 from 192.168.100.115
[root@k8s61 ~]# mkdir k8s
[root@k8s61 ~]# cd k8s/
[root@k8s61 k8s]# $ cat > kubeadm-config.yaml << EOF
> 
> apiVersion: kubeadm.k8s.io/v1beta2
> kind: InitConfiguration
> localAPIEndpoint:
>   advertiseAddress: 192.168.2.11
>   bindPort: 6443
> nodeRegistration:
>   taints:
>   - effect: PreferNoSchedule
>     key: node-role.kubernetes.io/master
> ---
> apiVersion: kubeadm.k8s.io/v1beta2
> kind: ClusterConfiguration
> imageRepository: registry.aliyuncs.com/google_containers
> kubernetesVersion: v1.16.3
> networking:
>   podSubnet: 10.244.0.0/16
> ---
> apiVersion: kubeproxy.config.k8s.io/v1alpha1
> kind: KubeProxyConfiguration
> mode: ipvs
> 
> $ cat > kubeadm-config.yaml << EOF

apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.2.11
  bindPort: 6443
nodeRegistration:
  taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
imageRepository: registry.aliyuncs.com/google_containers
kubernetesVersion: v1.16.3
networking:
  podSubnet: 10.244.0.0/16
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs

^C
[root@k8s61 k8s]# kubeadm config print init-defaults > kubeadm.conf
W0412 00:05:18.253431    1854 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[root@k8s61 k8s]# cat kubeadm.conf 
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s61
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.18.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
[root@k8s61 k8s]# vim kubeadm.conf 
[root@k8s61 k8s]# kubeadm config images list --config kubeadm.conf
W0412 00:11:17.719519    2240 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
registry.aliyuncs.com/google_containers/kube-apiserver:v1.18.0
registry.aliyuncs.com/google_containers/kube-controller-manager:v1.18.0
registry.aliyuncs.com/google_containers/kube-scheduler:v1.18.0
registry.aliyuncs.com/google_containers/kube-proxy:v1.18.0
registry.aliyuncs.com/google_containers/pause:3.2
registry.aliyuncs.com/google_containers/etcd:3.4.3-0
registry.aliyuncs.com/google_containers/coredns:1.6.7
[root@k8s61 k8s]# kubeadm init --config  kubeadm.conf 
W0412 00:12:59.410679    2358 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[init] Using Kubernetes version: v1.18.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s61 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.240.0.1 192.168.122.61]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s61 localhost] and IPs [192.168.122.61 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s61 localhost] and IPs [192.168.122.61 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
W0412 00:14:30.044861    2358 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[control-plane] Creating static Pod manifest for "kube-scheduler"
W0412 00:14:30.045841    2358 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 30.030288 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.18" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s61 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node k8s61 as control-plane by adding the taints [node-role.kubernetes.io/master:PreferNoSchedule]
[bootstrap-token] Using token: zt50dk.p3xmuj8rb11l37v1
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.122.61:6443 --token zt50dk.p3xmuj8rb11l37v1 \
    --discovery-token-ca-cert-hash sha256:d4e48912dd7fae4dead901711fa18879c8b9f4631c661b2b0cda166261deaf6f 
[root@k8s61 k8s]# mkdir -p $HOME/.kube
[root@k8s61 k8s]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@k8s61 k8s]# sudo chown $(id -u):$(id -g) $HOME/.kube/config
[root@k8s61 k8s]# echo "source <(kubectl completion bash)" >> ~/.bashrc
[root@k8s61 k8s]# wget https://docs.projectcalico.org/manifests/calico.yaml
-bash: wget: command not found
[root@k8s61 k8s]# yum install wget
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
Resolving Dependencies
--> Running transaction check
---> Package wget.x86_64 0:1.14-18.el7_6.1 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

=================================================================================================================================================================================== Package                                Arch                                     Version                                              Repository                              Size
===================================================================================================================================================================================Installing:
 wget                                   x86_64                                   1.14-18.el7_6.1                                      base                                   547 k

Transaction Summary
===================================================================================================================================================================================Install  1 Package

Total download size: 547 k
Installed size: 2.0 M
Is this ok [y/d/N]: y
Downloading packages:
wget-1.14-18.el7_6.1.x86_64.rpm                                                                                                                             | 547 kB  00:00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : wget-1.14-18.el7_6.1.x86_64                                                                                                                                     1/1 
  Verifying  : wget-1.14-18.el7_6.1.x86_64                                                                                                                                     1/1 

Installed:
  wget.x86_64 0:1.14-18.el7_6.1                                                                                                                                                    

Complete!
[root@k8s61 k8s]# wget https://docs.projectcalico.org/manifests/calico.yaml
--2020-04-12 00:18:38--  https://docs.projectcalico.org/manifests/calico.yaml
Resolving docs.projectcalico.org (docs.projectcalico.org)... 2400:6180:0:d1::4df:d001, 134.209.106.40
Connecting to docs.projectcalico.org (docs.projectcalico.org)|2400:6180:0:d1::4df:d001|:443... failed: Connection timed out.
Connecting to docs.projectcalico.org (docs.projectcalico.org)|134.209.106.40|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 21079 (21K) [application/x-yaml]
Saving to: ‘calico.yaml’

100%[=========================================================================================================================================>] 21,079      9.93KB/s   in 2.1s   

2020-04-12 00:20:50 (9.93 KB/s) - ‘calico.yaml’ saved [21079/21079]

[root@k8s61 k8s]# kubelet apply -f calico.yaml 
Usage:
  kubelet [flags]

Flags:
      --add-dir-header                                                                                            If true, adds the file directory to the header
      --address 0.0.0.0                                                                                           The IP address for the Kubelet to serve on (set to 0.0.0.0 for all IPv4 interfaces and `::` for all IPv6 interfaces) (default 0.0.0.0) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --allowed-unsafe-sysctls strings                                                                            Comma-separated whitelist of unsafe sysctls or unsafe sysctl patterns (ending in *). Use these at your own risk. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --alsologtostderr                                                                                           log to standard error as well as files
      --anonymous-auth                                                                                            Enables anonymous requests to the Kubelet server. Requests that are not rejected by another authentication method are treated as anonymous requests. Anonymous requests have a username of system:anonymous, and a group name of system:unauthenticated. (default true) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --application-metrics-count-limit int                                                                       Max number of application metrics to store (per container) (default 100) (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --authentication-token-webhook                                                                              Use the TokenReview API to determine authentication for bearer tokens. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --authentication-token-webhook-cache-ttl duration                                                           The duration to cache responses from the webhook token authenticator. (default 2m0s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --authorization-mode string                                                                                 Authorization mode for Kubelet server. Valid options are AlwaysAllow or Webhook. Webhook mode uses the SubjectAccessReview API to determine authorization. (default "AlwaysAllow") (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --authorization-webhook-cache-authorized-ttl duration                                                       The duration to cache 'authorized' responses from the webhook authorizer. (default 5m0s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --authorization-webhook-cache-unauthorized-ttl duration                                                     The duration to cache 'unauthorized' responses from the webhook authorizer. (default 30s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --azure-container-registry-config string                                                                    Path to the file containing Azure container registry configuration information.
      --boot-id-file string                                                                                       Comma-separated list of files to check for boot-id. Use the first one that exists. (default "/proc/sys/kernel/random/boot_id") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --bootstrap-checkpoint-path string                                                                          <Warning: Alpha feature> Path to the directory where the checkpoints are stored
      --bootstrap-kubeconfig string                                                                               Path to a kubeconfig file that will be used to get client certificate for kubelet. If the file specified by --kubeconfig does not exist, the bootstrap kubeconfig is used to request a client certificate from the API server. On success, a kubeconfig file referencing the generated client certificate and key is written to the path specified by --kubeconfig. The client certificate and key file will be stored in the directory pointed by --cert-dir.
      --cert-dir string                                                                                           The directory where the TLS certs are located. If --tls-cert-file and --tls-private-key-file are provided, this flag will be ignored. (default "/var/lib/kubelet/pki")
      --cgroup-driver string                                                                                      Driver that the kubelet uses to manipulate cgroups on the host.  Possible values: 'cgroupfs', 'systemd' (default "cgroupfs") (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --cgroup-root string                                                                                        Optional root cgroup to use for pods. This is handled by the container runtime on a best effort basis. Default: '', which means use the container runtime default. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --cgroups-per-qos                                                                                           Enable creation of QoS cgroup hierarchy, if true top level QoS and pod cgroups are created. (default true) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --chaos-chance float                                                                                        If > 0.0, introduce random client errors and latency. Intended for testing.
      --client-ca-file string                                                                                     If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --cloud-config string                                                                                       The path to the cloud provider configuration file.  Empty string for no configuration file.
      --cloud-provider string                                                                                     The provider for cloud services. Specify empty string for running with no cloud provider. If set, the cloud provider determines the name of the node (consult cloud provider documentation to determine if and how the hostname is used).
      --cluster-dns strings                                                                                       Comma-separated list of DNS server IP address.  This value is used for containers DNS server in case of Pods with "dnsPolicy=ClusterFirst". Note: all DNS servers appearing in the list MUST serve the same set of records otherwise name resolution within the cluster may not work correctly. There is no guarantee as to which DNS server may be contacted for name resolution. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --cluster-domain string                                                                                     Domain for this cluster.  If set, kubelet will configure all containers to search this domain in addition to the host's search domains (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --cni-bin-dir string                                                                                        <Warning: Alpha feature> A comma-separated list of full paths of directories in which to search for CNI plugin binaries. This docker-specific flag only works when container-runtime is set to docker. (default "/opt/cni/bin")
      --cni-cache-dir string                                                                                      <Warning: Alpha feature> The full path of the directory in which CNI should store cache files. This docker-specific flag only works when container-runtime is set to docker. (default "/var/lib/cni/cache")
      --cni-conf-dir string                                                                                       <Warning: Alpha feature> The full path of the directory in which to search for CNI config files. This docker-specific flag only works when container-runtime is set to docker. (default "/etc/cni/net.d")
      --config string                                                                                             The Kubelet will load its initial configuration from this file. The path may be absolute or relative; relative paths start at the Kubelet's current working directory. Omit this flag to use the built-in default configuration values. Command-line flags override configuration from this file.
      --container-hints string                                                                                    location of the container hints file (default "/etc/cadvisor/container_hints.json") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --container-log-max-files int32                                                                             <Warning: Beta feature> Set the maximum number of container log files that can be present for a container. The number must be >= 2. This flag can only be used with --container-runtime=remote. (default 5) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --container-log-max-size string                                                                             <Warning: Beta feature> Set the maximum size (e.g. 10Mi) of container log file before it is rotated. This flag can only be used with --container-runtime=remote. (default "10Mi") (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --container-runtime string                                                                                  The container runtime to use. Possible values: 'docker', 'remote'. (default "docker")
      --container-runtime-endpoint string                                                                         [Experimental] The endpoint of remote runtime service. Currently unix socket endpoint is supported on Linux, while npipe and tcp endpoints are supported on windows.  Examples:'unix:///var/run/dockershim.sock', 'npipe:////./pipe/dockershim' (default "unix:///var/run/dockershim.sock")
      --containerd string                                                                                         containerd endpoint (default "/run/containerd/containerd.sock") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --contention-profiling                                                                                      Enable lock contention profiling, if profiling is enabled (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --cpu-cfs-quota                                                                                             Enable CPU CFS quota enforcement for containers that specify CPU limits (default true) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --cpu-cfs-quota-period duration                                                                             Sets CPU CFS quota period value, cpu.cfs_period_us, defaults to Linux Kernel default (default 100ms) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --cpu-manager-policy string                                                                                 CPU Manager policy to use. Possible values: 'none', 'static'. Default: 'none' (default "none") (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --cpu-manager-reconcile-period NodeStatusUpdateFrequency                                                    <Warning: Alpha feature> CPU Manager reconciliation period. Examples: '10s', or '1m'. If not supplied, defaults to NodeStatusUpdateFrequency (default 10s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --docker string                                                                                             docker endpoint (default "unix:///var/run/docker.sock") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --docker-endpoint string                                                                                    Use this for the docker endpoint to communicate with. This docker-specific flag only works when container-runtime is set to docker. (default "unix:///var/run/docker.sock")
      --docker-env-metadata-whitelist string                                                                      a comma-separated list of environment variable keys that needs to be collected for docker containers (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --docker-only                                                                                               Only report docker containers in addition to root stats (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --docker-root string                                                                                        DEPRECATED: docker root is read from docker info (this is a fallback, default: /var/lib/docker) (default "/var/lib/docker")
      --docker-tls                                                                                                use TLS to connect to docker (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --docker-tls-ca string                                                                                      path to trusted CA (default "ca.pem") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --docker-tls-cert string                                                                                    path to client certificate (default "cert.pem") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --docker-tls-key string                                                                                     path to private key (default "key.pem") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --dynamic-config-dir string                                                                                 The Kubelet will use this directory for checkpointing downloaded configurations and tracking configuration health. The Kubelet will create this directory if it does not already exist. The path may be absolute or relative; relative paths start at the Kubelet's current working directory. Providing this flag enables dynamic Kubelet configuration. The DynamicKubeletConfig feature gate must be enabled to pass this flag; this gate currently defaults to true because the feature is beta.
      --enable-cadvisor-json-endpoints                                                                            Enable cAdvisor json /spec and /stats/* endpoints. (DEPRECATED: will be removed in a future version)
      --enable-controller-attach-detach                                                                           Enables the Attach/Detach controller to manage attachment/detachment of volumes scheduled to this node, and disables kubelet from executing any attach/detach operations (default true) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --enable-debugging-handlers                                                                                 Enables server endpoints for log collection and local running of containers and commands (default true) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --enable-load-reader                                                                                        Whether to enable cpu load reader (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --enable-server                                                                                             Enable the Kubelet's server (default true)
      --enforce-node-allocatable strings                                                                          A comma separated list of levels of node allocatable enforcement to be enforced by kubelet. Acceptable options are 'none', 'pods', 'system-reserved', and 'kube-reserved'. If the latter two options are specified, '--system-reserved-cgroup' and '--kube-reserved-cgroup' must also be set, respectively. If 'none' is specified, no additional options should be set. See https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ for more details. (default [pods]) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --event-burst int32                                                                                         Maximum size of a bursty event records, temporarily allows event records to burst to this number, while still not exceeding event-qps. Only used if --event-qps > 0 (default 10) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --event-qps int32                                                                                           If > 0, limit event creations per second to this value. If 0, unlimited. (default 5) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --event-storage-age-limit string                                                                            Max length of time for which to store events (per type). Value is a comma separated list of key values, where the keys are event types (e.g.: creation, oom) or "default" and the value is a duration. Default is applied to all non-specified event types (default "default=0") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --event-storage-event-limit string                                                                          Max number of events to store (per type). Value is a comma separated list of key values, where the keys are event types (e.g.: creation, oom) or "default" and the value is an integer. Default is applied to all non-specified event types (default "default=0") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --eviction-hard mapStringString                                                                             A set of eviction thresholds (e.g. memory.available<1Gi) that if met would trigger a pod eviction. (default imagefs.available<15%,memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --eviction-max-pod-grace-period int32                                                                       Maximum allowed grace period (in seconds) to use when terminating pods in response to a soft eviction threshold being met.  If negative, defer to pod specified value. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --eviction-minimum-reclaim mapStringString                                                                  A set of minimum reclaims (e.g. imagefs.available=2Gi) that describes the minimum amount of resource the kubelet will reclaim when performing a pod eviction if that resource is under pressure. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --eviction-pressure-transition-period duration                                                              Duration for which the kubelet has to wait before transitioning out of an eviction pressure condition. (default 5m0s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --eviction-soft mapStringString                                                                             A set of eviction thresholds (e.g. memory.available<1.5Gi) that if met over a corresponding grace period would trigger a pod eviction. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --eviction-soft-grace-period mapStringString                                                                A set of eviction grace periods (e.g. memory.available=1m30s) that correspond to how long a soft eviction threshold must hold before triggering a pod eviction. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --exit-on-lock-contention                                                                                   Whether kubelet should exit upon lock-file contention.
      --experimental-allocatable-ignore-eviction                                                                  When set to 'true', Hard Eviction Thresholds will be ignored while calculating Node Allocatable. See https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ for more details. [default=false]
      --experimental-bootstrap-kubeconfig string                                                                   (DEPRECATED: Use --bootstrap-kubeconfig)
      --experimental-check-node-capabilities-before-mount                                                         [Experimental] if set true, the kubelet will check the underlying node for required components (binaries, etc.) before performing the mount
      --experimental-kernel-memcg-notification                                                                    If enabled, the kubelet will integrate with the kernel memcg notification to determine if memory eviction thresholds are crossed rather than polling.
      --experimental-mounter-path string                                                                          [Experimental] Path of mounter binary. Leave empty to use the default mount.
      --fail-swap-on                                                                                              Makes the Kubelet fail to start if swap is enabled on the node.  (default true) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --feature-gates mapStringBool                                                                               A set of key=value pairs that describe feature gates for alpha/experimental features. Options are:
                APIListChunking=true|false (BETA - default=true)
                APIPriorityAndFairness=true|false (ALPHA - default=false)
                APIResponseCompression=true|false (BETA - default=true)
                AllAlpha=true|false (ALPHA - default=false)
                AllBeta=true|false (BETA - default=false)
                AllowInsecureBackendProxy=true|false (BETA - default=true)
                AnyVolumeDataSource=true|false (ALPHA - default=false)
                AppArmor=true|false (BETA - default=true)
                BalanceAttachedNodeVolumes=true|false (ALPHA - default=false)
                BoundServiceAccountTokenVolume=true|false (ALPHA - default=false)
                CPUManager=true|false (BETA - default=true)
                CRIContainerLogRotation=true|false (BETA - default=true)
                CSIInlineVolume=true|false (BETA - default=true)
                CSIMigration=true|false (BETA - default=true)
                CSIMigrationAWS=true|false (BETA - default=false)
                CSIMigrationAWSComplete=true|false (ALPHA - default=false)
                CSIMigrationAzureDisk=true|false (ALPHA - default=false)
                CSIMigrationAzureDiskComplete=true|false (ALPHA - default=false)
                CSIMigrationAzureFile=true|false (ALPHA - default=false)
                CSIMigrationAzureFileComplete=true|false (ALPHA - default=false)
                CSIMigrationGCE=true|false (BETA - default=false)
                CSIMigrationGCEComplete=true|false (ALPHA - default=false)
                CSIMigrationOpenStack=true|false (BETA - default=false)
                CSIMigrationOpenStackComplete=true|false (ALPHA - default=false)
                ConfigurableFSGroupPolicy=true|false (ALPHA - default=false)
                CustomCPUCFSQuotaPeriod=true|false (ALPHA - default=false)
                DefaultIngressClass=true|false (BETA - default=true)
                DevicePlugins=true|false (BETA - default=true)
                DryRun=true|false (BETA - default=true)
                DynamicAuditing=true|false (ALPHA - default=false)
                DynamicKubeletConfig=true|false (BETA - default=true)
                EndpointSlice=true|false (BETA - default=true)
                EndpointSliceProxying=true|false (ALPHA - default=false)
                EphemeralContainers=true|false (ALPHA - default=false)
                EvenPodsSpread=true|false (BETA - default=true)
                ExpandCSIVolumes=true|false (BETA - default=true)
                ExpandInUsePersistentVolumes=true|false (BETA - default=true)
                ExpandPersistentVolumes=true|false (BETA - default=true)
                ExperimentalHostUserNamespaceDefaulting=true|false (BETA - default=false)
                HPAScaleToZero=true|false (ALPHA - default=false)
                HugePageStorageMediumSize=true|false (ALPHA - default=false)
                HyperVContainer=true|false (ALPHA - default=false)
                IPv6DualStack=true|false (ALPHA - default=false)
                ImmutableEphemeralVolumes=true|false (ALPHA - default=false)
                KubeletPodResources=true|false (BETA - default=true)
                LegacyNodeRoleBehavior=true|false (ALPHA - default=true)
                LocalStorageCapacityIsolation=true|false (BETA - default=true)
                LocalStorageCapacityIsolationFSQuotaMonitoring=true|false (ALPHA - default=false)
                NodeDisruptionExclusion=true|false (ALPHA - default=false)
                NonPreemptingPriority=true|false (ALPHA - default=false)
                PodDisruptionBudget=true|false (BETA - default=true)
                PodOverhead=true|false (BETA - default=true)
                ProcMountType=true|false (ALPHA - default=false)
                QOSReserved=true|false (ALPHA - default=false)
                RemainingItemCount=true|false (BETA - default=true)
                RemoveSelfLink=true|false (ALPHA - default=false)
                ResourceLimitsPriorityFunction=true|false (ALPHA - default=false)
                RotateKubeletClientCertificate=true|false (BETA - default=true)
                RotateKubeletServerCertificate=true|false (BETA - default=true)
                RunAsGroup=true|false (BETA - default=true)
                RuntimeClass=true|false (BETA - default=true)
                SCTPSupport=true|false (ALPHA - default=false)
                SelectorIndex=true|false (ALPHA - default=false)
                ServerSideApply=true|false (BETA - default=true)
                ServiceAccountIssuerDiscovery=true|false (ALPHA - default=false)
                ServiceAppProtocol=true|false (ALPHA - default=false)
                ServiceNodeExclusion=true|false (ALPHA - default=false)
                ServiceTopology=true|false (ALPHA - default=false)
                StartupProbe=true|false (BETA - default=true)
                StorageVersionHash=true|false (BETA - default=true)
                SupportNodePidsLimit=true|false (BETA - default=true)
                SupportPodPidsLimit=true|false (BETA - default=true)
                Sysctls=true|false (BETA - default=true)
                TTLAfterFinished=true|false (ALPHA - default=false)
                TokenRequest=true|false (BETA - default=true)
                TokenRequestProjection=true|false (BETA - default=true)
                TopologyManager=true|false (BETA - default=true)
                ValidateProxyRedirects=true|false (BETA - default=true)
                VolumeSnapshotDataSource=true|false (BETA - default=true)
                WinDSR=true|false (ALPHA - default=false)
                WinOverlay=true|false (ALPHA - default=false) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --file-check-frequency duration                                                                             Duration between checking config files for new data (default 20s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --global-housekeeping-interval duration                                                                     Interval between global housekeepings (default 1m0s) (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --hairpin-mode string                                                                                       How should the kubelet setup hairpin NAT. This allows endpoints of a Service to loadbalance back to themselves if they should try to access their own Service. Valid values are "promiscuous-bridge", "hairpin-veth" and "none". (default "promiscuous-bridge") (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --healthz-bind-address 0.0.0.0                                                                              The IP address for the healthz server to serve on (set to 0.0.0.0 for all IPv4 interfaces and `::` for all IPv6 interfaces) (default 127.0.0.1) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --healthz-port int32                                                                                        The port of the localhost healthz endpoint (set to 0 to disable) (default 10248) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
  -h, --help                                                                                                      help for kubelet
      --hostname-override string                                                                                  If non-empty, will use this string as identification instead of the actual hostname. If --cloud-provider is set, the cloud provider determines the name of the node (consult cloud provider documentation to determine if and how the hostname is used).
      --housekeeping-interval duration                                                                            Interval between container housekeepings (default 10s)
      --http-check-frequency duration                                                                             Duration between checking http for new data (default 20s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --image-gc-high-threshold int32                                                                             The percent of disk usage after which image garbage collection is always run. Values must be within the range [0, 100], To disable image garbage collection, set to 100.  (default 85) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --image-gc-low-threshold int32                                                                              The percent of disk usage before which image garbage collection is never run. Lowest disk usage to garbage collect to. Values must be within the range [0, 100] and should not be larger than that of --image-gc-high-threshold. (default 80) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --image-pull-progress-deadline duration                                                                     If no pulling progress is made before this deadline, the image pulling will be cancelled. This docker-specific flag only works when container-runtime is set to docker. (default 1m0s)
      --image-service-endpoint string                                                                             [Experimental] The endpoint of remote image service. If not specified, it will be the same with container-runtime-endpoint by default. Currently unix socket endpoint is supported on Linux, while npipe and tcp endpoints are supported on windows.  Examples:'unix:///var/run/dockershim.sock', 'npipe:////./pipe/dockershim'
      --iptables-drop-bit int32                                                                                   The bit of the fwmark space to mark packets for dropping. Must be within the range [0, 31]. (default 15) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --iptables-masquerade-bit int32                                                                             The bit of the fwmark space to mark packets for SNAT. Must be within the range [0, 31]. Please match this parameter with corresponding parameter in kube-proxy. (default 14) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --keep-terminated-pod-volumes                                                                               Keep terminated pod volumes mounted to the node after the pod terminates.  Can be useful for debugging volume related issues. (DEPRECATED: will be removed in a future version)
      --kube-api-burst int32                                                                                      Burst to use while talking with kubernetes apiserver (default 10) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --kube-api-content-type string                                                                              Content type of requests sent to apiserver. (default "application/vnd.kubernetes.protobuf") (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --kube-api-qps int32                                                                                        QPS to use while talking with kubernetes apiserver (default 5) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --kube-reserved mapStringString                                                                             A set of ResourceName=ResourceQuantity (e.g. cpu=200m,memory=500Mi,ephemeral-storage=1Gi) pairs that describe resources reserved for kubernetes system components. Currently cpu, memory and local ephemeral storage for root file system are supported. See http://kubernetes.io/docs/user-guide/compute-resources for more detail. [default=none] (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --kube-reserved-cgroup string                                                                               Absolute name of the top level cgroup that is used to manage kubernetes components for which compute resources were reserved via '--kube-reserved' flag. Ex. '/kube-reserved'. [default=''] (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --kubeconfig string                                                                                         Path to a kubeconfig file, specifying how to connect to the API server. Providing --kubeconfig enables API server mode, omitting --kubeconfig enables standalone mode.
      --kubelet-cgroups string                                                                                    Optional absolute name of cgroups to create and run the Kubelet in. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --lock-file string                                                                                          <Warning: Alpha feature> The path to file for kubelet to use as a lock file.
      --log-backtrace-at traceLocation                                                                            when logging hits line file:N, emit a stack trace (default :0)
      --log-cadvisor-usage                                                                                        Whether to log the usage of the cAdvisor container (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --log-dir string                                                                                            If non-empty, write log files in this directory
      --log-file string                                                                                           If non-empty, use this log file
      --log-file-max-size uint                                                                                    Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)
      --log-flush-frequency duration                                                                              Maximum number of seconds between log flushes (default 5s)
      --logtostderr                                                                                               log to standard error instead of files (default true)
      --machine-id-file string                                                                                    Comma-separated list of files to check for machine-id. Use the first one that exists. (default "/etc/machine-id,/var/lib/dbus/machine-id") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --make-iptables-util-chains                                                                                 If true, kubelet will ensure iptables utility rules are present on host. (default true) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --manifest-url string                                                                                       URL for accessing additional Pod specifications to run (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --manifest-url-header --manifest-url-header 'a:hello,b:again,c:world' --manifest-url-header 'b:beautiful'   Comma-separated list of HTTP headers to use when accessing the url provided to --manifest-url. Multiple headers with the same name will be added in the same order provided. This flag can be repeatedly invoked. For example: --manifest-url-header 'a:hello,b:again,c:world' --manifest-url-header 'b:beautiful' (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --master-service-namespace string                                                                           The namespace from which the kubernetes master services should be injected into pods (default "default") (DEPRECATED: This flag will be removed in a future version.)
      --max-open-files int                                                                                        Number of files that can be opened by Kubelet process. (default 1000000) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --max-pods int32                                                                                            Number of Pods that can run on this Kubelet. (default 110) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --maximum-dead-containers int32                                                                             Maximum number of old instances of containers to retain globally.  Each container takes up some disk space. To disable, set to a negative number. (default -1) (DEPRECATED: Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.)
      --maximum-dead-containers-per-container int32                                                               Maximum number of old instances to retain per container.  Each container takes up some disk space. (default 1) (DEPRECATED: Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.)
      --minimum-container-ttl-duration duration                                                                   Minimum age for a finished container before it is garbage collected.  Examples: '300ms', '10s' or '2h45m' (DEPRECATED: Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.)
      --minimum-image-ttl-duration duration                                                                       Minimum age for an unused image before it is garbage collected.  Examples: '300ms', '10s' or '2h45m'. (default 2m0s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --network-plugin string                                                                                     <Warning: Alpha feature> The name of the network plugin to be invoked for various events in kubelet/pod lifecycle. This docker-specific flag only works when container-runtime is set to docker.
      --network-plugin-mtu int32                                                                                  <Warning: Alpha feature> The MTU to be passed to the network plugin, to override the default. Set to 0 to use the default 1460 MTU. This docker-specific flag only works when container-runtime is set to docker.
      --node-ip ::                                                                                                IP address of the node. If set, kubelet will use this IP address for the node. If unset, kubelet will use the node's default IPv4 address, if any, or its default IPv6 address if it has no IPv4 addresses. You can pass :: to make it prefer the default IPv6 address rather than the default IPv4 address.
      --node-labels mapStringString                                                                               <Warning: Alpha feature> Labels to add when registering the node in the cluster.  Labels must be key=value pairs separated by ','. Labels in the 'kubernetes.io' namespace must begin with an allowed prefix (kubelet.kubernetes.io, node.kubernetes.io) or be in the specifically allowed set (beta.kubernetes.io/arch, beta.kubernetes.io/instance-type, beta.kubernetes.io/os, failure-domain.beta.kubernetes.io/region, failure-domain.beta.kubernetes.io/zone, kubernetes.io/arch, kubernetes.io/hostname, kubernetes.io/os, node.kubernetes.io/instance-type, topology.kubernetes.io/region, topology.kubernetes.io/zone)
      --node-status-max-images int32                                                                              <Warning: Alpha feature> The maximum number of images to report in Node.Status.Images. If -1 is specified, no cap will be applied. (default 50)
      --node-status-update-frequency duration                                                                     Specifies how often kubelet posts node status to master. Note: be cautious when changing the constant, it must work with nodeMonitorGracePeriod in nodecontroller. (default 10s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --non-masquerade-cidr string                                                                                Traffic to IPs outside this range will use IP masquerade. Set to '0.0.0.0/0' to never masquerade. (default "10.0.0.0/8") (DEPRECATED: will be removed in a future version)
      --oom-score-adj int32                                                                                       The oom-score-adj value for kubelet process. Values must be within the range [-1000, 1000] (default -999) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --pod-cidr string                                                                                           The CIDR to use for pod IP addresses, only used in standalone mode.  In cluster mode, this is obtained from the master. For IPv6, the maximum number of IP's allocated is 65536 (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --pod-infra-container-image string                                                                          The image whose network/ipc namespaces containers in each pod will use. This docker-specific flag only works when container-runtime is set to docker. (default "k8s.gcr.io/pause:3.2")
      --pod-manifest-path string                                                                                  Path to the directory containing static pod files to run, or the path to a single static pod file. Files starting with dots will be ignored. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --pod-max-pids int                                                                                          Set the maximum number of processes per pod.  If -1, the kubelet defaults to the node allocatable pid capacity. (default -1) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --pods-per-core int32                                                                                       Number of Pods per core that can run on this Kubelet. The total number of Pods on this Kubelet cannot exceed max-pods, so max-pods will be used if this calculation results in a larger number of Pods allowed on the Kubelet. A value of 0 disables this limit. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --port int32                                                                                                The port for the Kubelet to serve on. (default 10250) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --protect-kernel-defaults                                                                                   Default kubelet behaviour for kernel tuning. If set, kubelet errors if any of kernel tunables is different than kubelet defaults. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --provider-id string                                                                                        Unique identifier for identifying the node in a machine database, i.e cloudprovider
      --qos-reserved mapStringString                                                                              <Warning: Alpha feature> A set of ResourceName=Percentage (e.g. memory=50%) pairs that describe how pod resource requests are reserved at the QoS level. Currently only memory is supported. Requires the QOSReserved feature gate to be enabled. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --read-only-port int32                                                                                      The read-only port for the Kubelet to serve on with no authentication/authorization (set to 0 to disable) (default 10255) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --really-crash-for-testing                                                                                  If true, when panics occur crash. Intended for testing.
      --redirect-container-streaming                                                                              Enables container streaming redirect. If false, kubelet will proxy container streaming data between apiserver and container runtime; if true, kubelet will return an http redirect to apiserver, and apiserver will access container runtime directly. The proxy approach is more secure, but introduces some overhead. The redirect approach is more performant, but less secure because the connection between apiserver and container runtime may not be authenticated. (DEPRECATED: Container streaming redirection will be removed from the kubelet in v1.20, and this flag will be removed in v1.22. For more details, see http://git.k8s.io/enhancements/keps/sig-node/20191205-container-streaming-requests.md)
      --register-node                                                                                             Register the node with the apiserver. If --kubeconfig is not provided, this flag is irrelevant, as the Kubelet won't have an apiserver to register with. (default true)
      --register-schedulable                                                                                      Register the node as schedulable. Won't have any effect if register-node is false. (default true) (DEPRECATED: will be removed in a future version)
      --register-with-taints []api.Taint                                                                          Register the node with the given list of taints (comma separated "<key>=<value>:<effect>"). No-op if register-node is false.
      --registry-burst int32                                                                                      Maximum size of a bursty pulls, temporarily allows pulls to burst to this number, while still not exceeding registry-qps. Only used if --registry-qps > 0 (default 10) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --registry-qps int32                                                                                        If > 0, limit registry pull QPS to this value.  If 0, unlimited. (default 5) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --reserved-cpus string                                                                                      A comma-separated list of CPUs or CPU ranges that are reserved for system and kubernetes usage. This specific list will supersede cpu counts in --system-reserved and --kube-reserved. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --resolv-conf string                                                                                        Resolver configuration file used as the basis for the container DNS resolution configuration. (default "/etc/resolv.conf") (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --root-dir string                                                                                           Directory path for managing kubelet files (volume mounts,etc). (default "/var/lib/kubelet")
      --rotate-certificates                                                                                       <Warning: Beta feature> Auto rotate the kubelet client certificates by requesting new certificates from the kube-apiserver when the certificate expiration approaches. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --rotate-server-certificates                                                                                Auto-request and rotate the kubelet serving certificates by requesting new certificates from the kube-apiserver when the certificate expiration approaches. Requires the RotateKubeletServerCertificate feature gate to be enabled, and approval of the submitted CertificateSigningRequest objects. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --runonce                                                                                                   If true, exit after spawning pods from static pod files or remote urls. Exclusive with --enable-server
      --runtime-cgroups string                                                                                    Optional absolute name of cgroups to create and run the runtime in.
      --runtime-request-timeout duration                                                                          Timeout of all runtime requests except long running request - pull, logs, exec and attach. When timeout exceeded, kubelet will cancel the request, throw out an error and retry later. (default 2m0s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --seccomp-profile-root string                                                                               <Warning: Alpha feature> Directory path for seccomp profiles. (default "/var/lib/kubelet/seccomp")
      --serialize-image-pulls                                                                                     Pull images one at a time. We recommend *not* changing the default value on nodes that run docker daemon with version < 1.9 or an Aufs storage backend. Issue #10959 has more details. (default true) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --skip-headers                                                                                              If true, avoid header prefixes in the log messages
      --skip-log-headers                                                                                          If true, avoid headers when opening log files
      --stderrthreshold severity                                                                                  logs at or above this threshold go to stderr (default 2)
      --storage-driver-buffer-duration duration                                                                   Writes in the storage driver will be buffered for this duration, and committed to the non memory backends as a single transaction (default 1m0s) (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --storage-driver-db string                                                                                  database name (default "cadvisor") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --storage-driver-host string                                                                                database host:port (default "localhost:8086") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --storage-driver-password string                                                                            database password (default "root") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --storage-driver-secure                                                                                     use secure connection with database (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --storage-driver-table string                                                                               table name (default "stats") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --storage-driver-user string                                                                                database username (default "root") (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.)
      --streaming-connection-idle-timeout duration                                                                Maximum time a streaming connection can be idle before the connection is automatically closed. 0 indicates no timeout. Example: '5m' (default 4h0m0s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --sync-frequency duration                                                                                   Max period between synchronizing running containers and config (default 1m0s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --system-cgroups /                                                                                          Optional absolute name of cgroups in which to place all non-kernel processes that are not already inside a cgroup under /. Empty for no container. Rolling back the flag requires a reboot. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --system-reserved mapStringString                                                                           A set of ResourceName=ResourceQuantity (e.g. cpu=200m,memory=500Mi,ephemeral-storage=1Gi) pairs that describe resources reserved for non-kubernetes components. Currently only cpu and memory are supported. See http://kubernetes.io/docs/user-guide/compute-resources for more detail. [default=none] (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --system-reserved-cgroup string                                                                             Absolute name of the top level cgroup that is used to manage non-kubernetes components for which compute resources were reserved via '--system-reserved' flag. Ex. '/system-reserved'. [default=''] (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --tls-cert-file string                                                                                      File containing x509 Certificate used for serving HTTPS (with intermediate certs, if any, concatenated after server cert). If --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory passed to --cert-dir. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --tls-cipher-suites strings                                                                                 Comma-separated list of cipher suites for the server. If omitted, the default Go cipher suites will be used. Possible values: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_RC4_128_SHA,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_RC4_128_SHA,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_RC4_128_SHA (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --tls-min-version string                                                                                    Minimum TLS version supported. Possible values: VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13 (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --tls-private-key-file string                                                                               File containing x509 private key matching --tls-cert-file. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
      --topology-manager-policy string                                                                            Topology Manager policy to use. Possible values: 'none', 'best-effort', 'restricted', 'single-numa-node'. (default "none") (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
  -v, --v Level                                                                                                   number for the log level verbosity
      --version version[=true]                                                                                    Print version information and quit
      --vmodule moduleSpec                                                                                        comma-separated list of pattern=N settings for file-filtered logging
      --volume-plugin-dir string                                                                                  The full path of the directory in which to search for additional third party volume plugins (default "/usr/libexec/kubernetes/kubelet-plugins/volume/exec/")
      --volume-stats-agg-period duration                                                                          Specifies interval for kubelet to calculate and cache the volume disk usage for all pods and volumes.  To disable volume calculations, set to 0. (default 1m0s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)
F0412 00:21:36.981170    6546 server.go:157] unknown shorthand flag: 'f' in -f
[root@k8s61 k8s]# kubectl apply -f calico.yaml 
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
[root@k8s61 k8s]# echo "source <(kubectl completion bash)" >> ~/.bashrc
[root@k8s61 k8s]# kubectl 
calico.yaml   kubeadm.conf  
[root@k8s61 k8s]# kubectl get pods -A
NAMESPACE     NAME                                       READY   STATUS     RESTARTS   AGE
kube-system   calico-kube-controllers-555fc8cc5c-sbcr4   0/1     Pending    0          30s
kube-system   calico-node-hsspv                          0/1     Init:0/3   0          31s
kube-system   coredns-7ff77c879f-7mxwm                   0/1     Pending    0          7m44s
kube-system   coredns-7ff77c879f-nh628                   0/1     Pending    0          7m44s
kube-system   etcd-k8s61                                 1/1     Running    0          7m42s
kube-system   kube-apiserver-k8s61                       1/1     Running    0          7m42s
kube-system   kube-controller-manager-k8s61              1/1     Running    0          7m42s
kube-system   kube-proxy-h9dsf                           1/1     Running    0          7m44s
kube-system   kube-scheduler-k8s61                       1/1     Running    0          7m42s
[root@k8s61 k8s]# kubectl get pods -A
NAMESPACE     NAME                                       READY   STATUS                  RESTARTS   AGE
kube-system   calico-kube-controllers-555fc8cc5c-sbcr4   0/1     Pending                 0          118s
kube-system   calico-node-hsspv                          0/1     Init:ImagePullBackOff   0          119s
kube-system   coredns-7ff77c879f-7mxwm                   0/1     Pending                 0          9m12s
kube-system   coredns-7ff77c879f-nh628                   0/1     Pending                 0          9m12s
kube-system   etcd-k8s61                                 1/1     Running                 0          9m10s
kube-system   kube-apiserver-k8s61                       1/1     Running                 0          9m10s
kube-system   kube-controller-manager-k8s61              1/1     Running                 0          9m10s
kube-system   kube-proxy-h9dsf                           1/1     Running                 0          9m12s
kube-system   kube-scheduler-k8s61                       1/1     Running                 0          9m10s
[root@k8s61 k8s]# watch kubectl get pods --all-namespaces
[root@k8s61 k8s]# vim calico.yaml 
[root@k8s61 k8s]# vim kubeadm.conf 
[root@k8s61 k8s]# vim calico.yaml 
[root@k8s61 k8s]# kubectl apply -f calico.yaml 
configmap/calico-config unchanged
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org unchanged
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrole.rbac.authorization.k8s.io/calico-node unchanged
clusterrolebinding.rbac.authorization.k8s.io/calico-node unchanged
error: error parsing calico.yaml: error converting YAML to JSON: yaml: line 160: mapping values are not allowed in this context
[root@k8s61 k8s]# vim calico.yaml 
[root@k8s61 k8s]# vim calico.yaml 
[root@k8s61 k8s]# kubectl apply -f calico.yaml 
configmap/calico-config unchanged
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org unchanged
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org unchanged
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrole.rbac.authorization.k8s.io/calico-node unchanged
clusterrolebinding.rbac.authorization.k8s.io/calico-node unchanged
error: error parsing calico.yaml: error converting YAML to JSON: yaml: line 160: mapping values are not allowed in this context
[root@k8s61 k8s]# kubectl delete -f calico.yaml 
configmap "calico-config" deleted
customresourcedefinition.apiextensions.k8s.io "bgpconfigurations.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "bgppeers.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "blockaffinities.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "clusterinformations.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "felixconfigurations.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "globalnetworkpolicies.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "globalnetworksets.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "hostendpoints.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "ipamblocks.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "ipamconfigs.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "ipamhandles.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "ippools.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "networkpolicies.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "networksets.crd.projectcalico.org" deleted
clusterrole.rbac.authorization.k8s.io "calico-kube-controllers" deleted
clusterrolebinding.rbac.authorization.k8s.io "calico-kube-controllers" deleted
clusterrole.rbac.authorization.k8s.io "calico-node" deleted
clusterrolebinding.rbac.authorization.k8s.io "calico-node" deleted
error: error parsing calico.yaml: error converting YAML to JSON: yaml: line 160: mapping values are not allowed in this context
[root@k8s61 k8s]# vim kubeadm.conf 
[root@k8s61 k8s]# kubectl get pods -o wide -A
NAMESPACE     NAME                                       READY   STATUS                  RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
kube-system   calico-kube-controllers-555fc8cc5c-sbcr4   0/1     Pending                 0          12m   <none>           <none>   <none>           <none>
kube-system   calico-node-hsspv                          0/1     Init:ImagePullBackOff   0          12m   192.168.122.61   k8s61    <none>           <none>
kube-system   coredns-7ff77c879f-7mxwm                   0/1     Pending                 0          19m   <none>           <none>   <none>           <none>
kube-system   coredns-7ff77c879f-nh628                   0/1     Pending                 0          19m   <none>           <none>   <none>           <none>
kube-system   etcd-k8s61                                 1/1     Running                 0          19m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-apiserver-k8s61                       1/1     Running                 0          19m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-controller-manager-k8s61              1/1     Running                 0          19m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-proxy-h9dsf                           1/1     Running                 0          19m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-scheduler-k8s61                       1/1     Running                 0          19m   192.168.122.61   k8s61    <none>           <none>
[root@k8s61 k8s]# vim calico.yaml 
[root@k8s61 k8s]# kubectl delete -f calico.yaml 
daemonset.apps "calico-node" deleted
serviceaccount "calico-node" deleted
deployment.apps "calico-kube-controllers" deleted
serviceaccount "calico-kube-controllers" deleted
Error from server (NotFound): error when deleting "calico.yaml": configmaps "calico-config" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "bgpconfigurations.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "bgppeers.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "blockaffinities.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "clusterinformations.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "felixconfigurations.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "globalnetworkpolicies.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "globalnetworksets.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "hostendpoints.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "ipamblocks.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "ipamconfigs.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "ipamhandles.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "ippools.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "networkpolicies.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": customresourcedefinitions.apiextensions.k8s.io "networksets.crd.projectcalico.org" not found
Error from server (NotFound): error when deleting "calico.yaml": clusterroles.rbac.authorization.k8s.io "calico-kube-controllers" not found
Error from server (NotFound): error when deleting "calico.yaml": clusterrolebindings.rbac.authorization.k8s.io "calico-kube-controllers" not found
Error from server (NotFound): error when deleting "calico.yaml": clusterroles.rbac.authorization.k8s.io "calico-node" not found
Error from server (NotFound): error when deleting "calico.yaml": clusterrolebindings.rbac.authorization.k8s.io "calico-node" not found
[root@k8s61 k8s]# kubectl get pods -o wide -A
NAMESPACE     NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
kube-system   coredns-7ff77c879f-7mxwm        0/1     Pending   0          20m   <none>           <none>   <none>           <none>
kube-system   coredns-7ff77c879f-nh628        0/1     Pending   0          20m   <none>           <none>   <none>           <none>
kube-system   etcd-k8s61                      1/1     Running   0          20m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-apiserver-k8s61            1/1     Running   0          20m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-controller-manager-k8s61   1/1     Running   0          20m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-proxy-h9dsf                1/1     Running   0          20m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-scheduler-k8s61            1/1     Running   0          20m   192.168.122.61   k8s61    <none>           <none>
[root@k8s61 k8s]# vim calico.yaml 
[root@k8s61 k8s]# kubectl apply -f calico.yaml 
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
[root@k8s61 k8s]# watch kubectl get pods --all-namespaces
[root@k8s61 k8s]# watch kubectl get pods --all-namespaces -o wide
[root@k8s61 k8s]# watch kubectl get pods --all-namespaces -o wide
[root@k8s61 k8s]# kubectl describe
error: You must specify the type of resource to describe. Use "kubectl api-resources" for a complete list of supported resources.
[root@k8s61 k8s]# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                       READY   STATUS     RESTARTS   AGE    IP               NODE     NOMINATED NODE   READINESS GATES
kube-system   calico-kube-controllers-555fc8cc5c-llw84   0/1     Pending    0          5m3s   <none>           <none>   <none>           <none>
kube-system   calico-node-hmsqx                          0/1     Init:0/3   0          5m3s   192.168.122.61   k8s61    <none>           <none>
kube-system   coredns-7ff77c879f-7mxwm                   0/1     Pending    0          27m    <none>           <none>   <none>           <none>
kube-system   coredns-7ff77c879f-nh628                   0/1     Pending    0          27m    <none>           <none>   <none>           <none>
kube-system   etcd-k8s61                                 1/1     Running    0          27m    192.168.122.61   k8s61    <none>           <none>
kube-system   kube-apiserver-k8s61                       1/1     Running    0          27m    192.168.122.61   k8s61    <none>           <none>
kube-system   kube-controller-manager-k8s61              1/1     Running    0          27m    192.168.122.61   k8s61    <none>           <none>
kube-system   kube-proxy-h9dsf                           1/1     Running    0          27m    192.168.122.61   k8s61    <none>           <none>
kube-system   kube-scheduler-k8s61                       1/1     Running    0          27m    192.168.122.61   k8s61    <none>           <none>
[root@k8s61 k8s]# kubectl logs calico-node-hmsqx
Error from server (NotFound): pods "calico-node-hmsqx" not found
[root@k8s61 k8s]# kubectl logs calico-node-hmsqx -n kube-system
Error from server (BadRequest): container "calico-node" in pod "calico-node-hmsqx" is waiting to start: PodInitializing
[root@k8s61 k8s]# kubectl describe pods calico-node-hmsqx -n kube-system
Name:                 calico-node-hmsqx
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 k8s61/192.168.122.61
Start Time:           Sun, 12 Apr 2020 00:37:11 +0800
Labels:               controller-revision-hash=7f78657bdf
                      k8s-app=calico-node
                      pod-template-generation=1
Annotations:          scheduler.alpha.kubernetes.io/critical-pod: 
Status:               Pending
IP:                   192.168.122.61
IPs:
  IP:           192.168.122.61
Controlled By:  DaemonSet/calico-node
Init Containers:
  upgrade-ipam:
    Container ID:  
    Image:         calico/cni:v3.13.2
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/cni/bin/calico-ipam
      -upgrade
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      KUBERNETES_NODE_NAME:        (v1:spec.nodeName)
      CALICO_NETWORKING_BACKEND:  <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
    Mounts:
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/lib/cni/networks from host-local-net-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-fw4jh (ro)
  install-cni:
    Container ID:  
    Image:         calico/cni:v3.13.2
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /install-cni.sh
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
      CNI_MTU:               <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false
      SLEEP:                 false
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-fw4jh (ro)
  flexvol-driver:
    Container ID:   
    Image:          calico/pod2daemon-flexvol:v3.13.2
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /host/driver from flexvol-driver-host (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-fw4jh (ro)
Containers:
  calico-node:
    Container ID:   
    Image:          calico/node:v3.13.2
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:      250m
    Liveness:   exec [/bin/calico-node -felix-live -bird-live] delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  exec [/bin/calico-node -felix-ready -bird-ready] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      WAIT_FOR_DATASTORE:                 true
      NODENAME:                            (v1:spec.nodeName)
      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
      CLUSTER_TYPE:                       k8s,bgp
      IP:                                 autodetect
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPMTU:                    <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false
      CALICO_IPV4POOL_CIDR:               10.244.0.0/12
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_LOGSEVERITYSCREEN:            info
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/calico from var-lib-calico (rw)
      /var/run/calico from var-run-calico (rw)
      /var/run/nodeagent from policysync (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-fw4jh (ro)
Conditions:
  Type              Status
  Initialized       False 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  var-run-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/calico
    HostPathType:  
  var-lib-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/calico
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  cni-bin-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/cni/bin
    HostPathType:  
  cni-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  host-local-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/networks
    HostPathType:  
  policysync:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/nodeagent
    HostPathType:  DirectoryOrCreate
  flexvol-driver-host:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
    HostPathType:  DirectoryOrCreate
  calico-node-token-fw4jh:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-node-token-fw4jh
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     :NoSchedule
                 :NoExecute
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  6m3s  default-scheduler  Successfully assigned kube-system/calico-node-hmsqx to k8s61
  Normal  Pulling    6m1s  kubelet, k8s61     Pulling image "calico/cni:v3.13.2"
[root@k8s61 k8s]# kubectl delete -f calico.yaml 
configmap "calico-config" deleted
customresourcedefinition.apiextensions.k8s.io "bgpconfigurations.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "bgppeers.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "blockaffinities.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "clusterinformations.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "felixconfigurations.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "globalnetworkpolicies.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "globalnetworksets.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "hostendpoints.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "ipamblocks.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "ipamconfigs.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "ipamhandles.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "ippools.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "networkpolicies.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "networksets.crd.projectcalico.org" deleted
clusterrole.rbac.authorization.k8s.io "calico-kube-controllers" deleted
clusterrolebinding.rbac.authorization.k8s.io "calico-kube-controllers" deleted
clusterrole.rbac.authorization.k8s.io "calico-node" deleted
clusterrolebinding.rbac.authorization.k8s.io "calico-node" deleted
daemonset.apps "calico-node" deleted
serviceaccount "calico-node" deleted
deployment.apps "calico-kube-controllers" deleted
serviceaccount "calico-kube-controllers" deleted
[root@k8s61 k8s]# 
[root@k8s61 k8s]# 
[root@k8s61 k8s]# 
[root@k8s61 k8s]# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
kube-system   coredns-7ff77c879f-7mxwm        0/1     Pending   0          31m   <none>           <none>   <none>           <none>
kube-system   coredns-7ff77c879f-nh628        0/1     Pending   0          31m   <none>           <none>   <none>           <none>
kube-system   etcd-k8s61                      1/1     Running   0          31m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-apiserver-k8s61            1/1     Running   0          31m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-controller-manager-k8s61   1/1     Running   0          31m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-proxy-h9dsf                1/1     Running   0          31m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-scheduler-k8s61            1/1     Running   0          31m   192.168.122.61   k8s61    <none>           <none>
[root@k8s61 k8s]# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.240.0.1   <none>        443/TCP   31m
[root@k8s61 k8s]# Last login: Sun Apr 12 00:03:36 2020 from 192.168.100.115
[root@k8s61 ~]# systemctl status kubelet.service 
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: activating (auto-restart) (Result: exit-code) since Sun 2020-04-12 00:12:45 CST; 5s ago
     Docs: https://kubernetes.io/docs/
  Process: 2362 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=255)
 Main PID: 2362 (code=exited, status=255)

Apr 12 00:12:45 k8s61 systemd[1]: kubelet.service: main process exited, code=exited, status=255/n/a
Apr 12 00:12:45 k8s61 systemd[1]: Unit kubelet.service entered failed state.
Apr 12 00:12:45 k8s61 systemd[1]: kubelet.service failed.
[root@k8s61 ~]# kubeadm config print init-defaults > kubeadm.conf
W0412 00:13:12.244889    2400 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[root@k8s61 ~]# vim kubeadm.conf 
[root@k8s61 ~]# rm kubeadm.conf 
rm: remove regular file ‘kubeadm.conf’? y
[root@k8s61 ~]# mkidr k8s
-bash: mkidr: command not found
[root@k8s61 ~]# mkdir k8s
mkdir: cannot create directory ‘k8s’: File exists
[root@k8s61 ~]# cd k8s/
[root@k8s61 k8s]# ls
kubeadm.conf
[root@k8s61 k8s]# vim kubeadm.conf 
[root@k8s61 k8s]# kubeadm config images list --config kubeadm.conf
W0412 00:15:03.567709    2564 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
registry.aliyuncs.com/google_containers/kube-apiserver:v1.18.0
registry.aliyuncs.com/google_containers/kube-controller-manager:v1.18.0
registry.aliyuncs.com/google_containers/kube-scheduler:v1.18.0
registry.aliyuncs.com/google_containers/kube-proxy:v1.18.0
registry.aliyuncs.com/google_containers/pause:3.2
registry.aliyuncs.com/google_containers/etcd:3.4.3-0
registry.aliyuncs.com/google_containers/coredns:1.6.7
[root@k8s61 k8s]# kubeadm config images pull --config kubeadm.conf
W0412 00:15:18.081792    2583 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.18.0
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.18.0
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.18.0
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.18.0
[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.2
[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.4.3-0
[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:1.6.7
[root@k8s61 k8s]# kubeadm init --config kubeadm.conf 
W0412 00:16:58.210654    2929 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[init] Using Kubernetes version: v1.18.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s61 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.244.0.1 192.168.122.61]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s61 localhost] and IPs [192.168.122.61 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s61 localhost] and IPs [192.168.122.61 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
W0412 00:17:04.996268    2929 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[control-plane] Creating static Pod manifest for "kube-scheduler"
W0412 00:17:04.997334    2929 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 29.503150 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.18" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s61 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node k8s61 as control-plane by adding the taints [node-role.kubernetes.io/master:PreferNoSchedule]
[bootstrap-token] Using token: oo222i.vhl2fjc0zf4dj1rz
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.122.61:6443 --token oo222i.vhl2fjc0zf4dj1rz \
    --discovery-token-ca-cert-hash sha256:3dbd745e8be24e66c85f0b85ac430dbad1252ed989e3c9f4891735e73dc9bd6d 
[root@k8s61 k8s]#   mkdir -p $HOME/.kube
[root@k8s61 k8s]#   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@k8s61 k8s]#   sudo chown $(id -u):$(id -g) $HOME/.kube/config
[root@k8s61 k8s]# ll -A ~/.kube/
total 8
-rw------- 1 root root 5450 Apr 12 00:18 config
[root@k8s61 k8s]# echo "source <(kubectl completion bash)" >> ~/.bashrc
[root@k8s61 k8s]# kubectl get pods -A -o wide
NAMESPACE     NAME                            READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
kube-system   coredns-7ff77c879f-gkvb8        0/1     Pending   0          2m11s   <none>           <none>   <none>           <none>
kube-system   coredns-7ff77c879f-vvpn4        0/1     Pending   0          2m11s   <none>           <none>   <none>           <none>
kube-system   etcd-k8s61                      1/1     Running   0          2m10s   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-apiserver-k8s61            1/1     Running   0          2m10s   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-controller-manager-k8s61   1/1     Running   0          2m10s   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-proxy-5gzww                1/1     Running   0          2m11s   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-scheduler-k8s61            1/1     Running   0          2m10s   192.168.122.61   k8s61    <none>           <none>
[root@k8s61 k8s]# systemctl status kubelet.service 
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active (running) since Sun 2020-04-12 00:17:36 CST; 2min 27s ago
     Docs: https://kubernetes.io/docs/
 Main PID: 4370 (kubelet)
    Tasks: 19
   Memory: 33.0M
   CGroup: /system.slice/kubelet.service
           └─4370 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml...
Apr 12 00:19:41 k8s61 kubelet[4370]: W0412 00:19:41.449896    4370 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Apr 12 00:19:43 k8s61 kubelet[4370]: E0412 00:19:43.203078    4370 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotR...ninitializedApr 12 00:19:46 k8s61 kubelet[4370]: W0412 00:19:46.450347    4370 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Apr 12 00:19:48 k8s61 kubelet[4370]: E0412 00:19:48.214662    4370 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotR...ninitializedApr 12 00:19:51 k8s61 kubelet[4370]: W0412 00:19:51.450747    4370 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Apr 12 00:19:53 k8s61 kubelet[4370]: E0412 00:19:53.227544    4370 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotR...ninitializedApr 12 00:19:56 k8s61 kubelet[4370]: W0412 00:19:56.451299    4370 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Apr 12 00:19:58 k8s61 kubelet[4370]: E0412 00:19:58.239114    4370 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotR...ninitializedApr 12 00:20:01 k8s61 kubelet[4370]: W0412 00:20:01.451792    4370 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Apr 12 00:20:03 k8s61 kubelet[4370]: E0412 00:20:03.251135    4370 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotR...ninitializedHint: Some lines were ellipsized, use -l to show in full.
[root@k8s61 k8s]# systemctl status kubelet.service -l
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active (running) since Sun 2020-04-12 00:17:36 CST; 2min 32s ago
     Docs: https://kubernetes.io/docs/
 Main PID: 4370 (kubelet)
    Tasks: 19
   Memory: 33.0M
   CGroup: /system.slice/kubelet.service
           └─4370 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=systemd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.2 --cgroup-driver=systemd

Apr 12 00:19:46 k8s61 kubelet[4370]: W0412 00:19:46.450347    4370 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Apr 12 00:19:48 k8s61 kubelet[4370]: E0412 00:19:48.214662    4370 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Apr 12 00:19:51 k8s61 kubelet[4370]: W0412 00:19:51.450747    4370 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Apr 12 00:19:53 k8s61 kubelet[4370]: E0412 00:19:53.227544    4370 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Apr 12 00:19:56 k8s61 kubelet[4370]: W0412 00:19:56.451299    4370 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Apr 12 00:19:58 k8s61 kubelet[4370]: E0412 00:19:58.239114    4370 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Apr 12 00:20:01 k8s61 kubelet[4370]: W0412 00:20:01.451792    4370 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Apr 12 00:20:03 k8s61 kubelet[4370]: E0412 00:20:03.251135    4370 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Apr 12 00:20:06 k8s61 kubelet[4370]: W0412 00:20:06.452371    4370 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Apr 12 00:20:08 k8s61 kubelet[4370]: E0412 00:20:08.263417    4370 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
[root@k8s61 k8s]# kubectl apply -f calico.yaml 
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
[root@k8s61 k8s]# watch kubectl get pods -A -o wide
[root@k8s61 k8s]# kubectl delete -f calico.yaml 
configmap "calico-config" deleted
customresourcedefinition.apiextensions.k8s.io "bgpconfigurations.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "bgppeers.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "blockaffinities.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "clusterinformations.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "felixconfigurations.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "globalnetworkpolicies.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "globalnetworksets.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "hostendpoints.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "ipamblocks.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "ipamconfigs.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "ipamhandles.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "ippools.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "networkpolicies.crd.projectcalico.org" deleted
customresourcedefinition.apiextensions.k8s.io "networksets.crd.projectcalico.org" deleted
clusterrole.rbac.authorization.k8s.io "calico-kube-controllers" deleted
clusterrolebinding.rbac.authorization.k8s.io "calico-kube-controllers" deleted
clusterrole.rbac.authorization.k8s.io "calico-node" deleted
clusterrolebinding.rbac.authorization.k8s.io "calico-node" deleted
daemonset.apps "calico-node" deleted
serviceaccount "calico-node" deleted
deployment.apps "calico-kube-controllers" deleted
serviceaccount "calico-kube-controllers" deleted
[root@k8s61 k8s]# yum install wget
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
Resolving Dependencies
--> Running transaction check
---> Package wget.x86_64 0:1.14-18.el7_6.1 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

=================================================================================================================================================================================== Package                                Arch                                     Version                                              Repository                              Size
===================================================================================================================================================================================Installing:
 wget                                   x86_64                                   1.14-18.el7_6.1                                      base                                   547 k

Transaction Summary
===================================================================================================================================================================================Install  1 Package

Total download size: 547 k
Installed size: 2.0 M
Is this ok [y/d/N]: y
Downloading packages:
wget-1.14-18.el7_6.1.x86_64.rpm                                                                                                                             | 547 kB  00:00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : wget-1.14-18.el7_6.1.x86_64                                                                                                                                     1/1 
  Verifying  : wget-1.14-18.el7_6.1.x86_64                                                                                                                                     1/1 

Installed:
  wget.x86_64 0:1.14-18.el7_6.1                                                                                                                                                    

Complete!
[root@k8s61 k8s]# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
--2020-04-12 00:26:43--  https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 14416 (14K) [text/plain]
Saving to: ‘kube-flannel.yml’

100%[=========================================================================================================================================>] 14,416      4.55KB/s   in 3.1s   

2020-04-12 00:26:47 (4.55 KB/s) - ‘kube-flannel.yml’ saved [14416/14416]

[root@k8s61 k8s]# kubectl apply -f kube
kubeadm.conf      kube-flannel.yml  
[root@k8s61 k8s]# kubectl apply -f kube-flannel.yml 
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds-amd64 created
daemonset.apps/kube-flannel-ds-arm64 created
daemonset.apps/kube-flannel-ds-arm created
daemonset.apps/kube-flannel-ds-ppc64le created
daemonset.apps/kube-flannel-ds-s390x created
[root@k8s61 k8s]# watch kubectl get pods -A -o wide
[root@k8s61 k8s]# kubectl delete -f kube-flannel.yml 
podsecuritypolicy.policy "psp.flannel.unprivileged" deleted
clusterrole.rbac.authorization.k8s.io "flannel" deleted
clusterrolebinding.rbac.authorization.k8s.io "flannel" deleted
serviceaccount "flannel" deleted
configmap "kube-flannel-cfg" deleted
daemonset.apps "kube-flannel-ds-amd64" deleted
daemonset.apps "kube-flannel-ds-arm64" deleted
daemonset.apps "kube-flannel-ds-arm" deleted
daemonset.apps "kube-flannel-ds-ppc64le" deleted
daemonset.apps "kube-flannel-ds-s390x" deleted
[root@k8s61 k8s]# vim kube-flannel.yml 
[root@k8s61 k8s]# vim kube-flannel.yml 
[root@k8s61 k8s]# rm -rf /var/lib/cni/
[root@k8s61 k8s]# rm -f /etc/cni/net.d/*
[root@k8s61 k8s]# systemctl restart kubelet.service 
[root@k8s61 k8s]# systemctl status kubelet.service 
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active (running) since Sun 2020-04-12 00:31:48 CST; 7s ago
     Docs: https://kubernetes.io/docs/
 Main PID: 12675 (kubelet)
    Tasks: 19
   Memory: 38.3M
   CGroup: /system.slice/kubelet.service
           └─12675 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yam...
Apr 12 00:31:55 k8s61 kubelet[12675]: I0412 00:31:55.555717   12675 topology_manager.go:233] [topologymanager] Topology Admit Handler
Apr 12 00:31:55 k8s61 kubelet[12675]: E0412 00:31:55.565643   12675 kubelet.go:1663] Failed creating a mirror pod for "kube-apiserver-k8s61_kube-system(7f3788359367...ready existsApr 12 00:31:55 k8s61 kubelet[12675]: E0412 00:31:55.565764   12675 kubelet.go:1663] Failed creating a mirror pod for "kube-controller-manager-k8s61_kube-system(015...ready existsApr 12 00:31:55 k8s61 kubelet[12675]: I0412 00:31:55.586118   12675 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-pro...1f50f590bf")Apr 12 00:31:55 k8s61 kubelet[12675]: I0412 00:31:55.586334   12675 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "xtables-...1f50f590bf")Apr 12 00:31:55 k8s61 kubelet[12675]: I0412 00:31:55.586365   12675 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconf...6c7d883bbe")Apr 12 00:31:55 k8s61 kubelet[12675]: I0412 00:31:55.586492   12675 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "lib-modu...1f50f590bf")Apr 12 00:31:55 k8s61 kubelet[12675]: I0412 00:31:55.586516   12675 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy-token-s8g...Apr 12 00:31:55 k8s61 kubelet[12675]: I0412 00:31:55.686990   12675 reconciler.go:157] Reconciler: start to sync state
Apr 12 00:31:55 k8s61 kubelet[12675]: E0412 00:31:55.709853   12675 kubelet.go:1663] Failed creating a mirror pod for "kube-scheduler-k8s61_kube-system(ca2aa1b3224c...ready existsHint: Some lines were ellipsized, use -l to show in full.
[root@k8s61 k8s]# systemctl status kubelet.service  -l
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active (running) since Sun 2020-04-12 00:31:48 CST; 10s ago
     Docs: https://kubernetes.io/docs/
 Main PID: 12675 (kubelet)
    Tasks: 19
   Memory: 37.5M
   CGroup: /system.slice/kubelet.service
           └─12675 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=systemd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.2 --cgroup-driver=systemd

Apr 12 00:31:55 k8s61 kubelet[12675]: E0412 00:31:55.565643   12675 kubelet.go:1663] Failed creating a mirror pod for "kube-apiserver-k8s61_kube-system(7f378835936728b6fcbb57eb6c3eaa66)": pods "kube-apiserver-k8s61" already exists
Apr 12 00:31:55 k8s61 kubelet[12675]: E0412 00:31:55.565764   12675 kubelet.go:1663] Failed creating a mirror pod for "kube-controller-manager-k8s61_kube-system(0152186eab17b9a2d5c82c2626e4e64b)": pods "kube-controller-manager-k8s61" already exists
Apr 12 00:31:55 k8s61 kubelet[12675]: I0412 00:31:55.586118   12675 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy" (UniqueName: "kubernetes.io/configmap/942c436a-941e-43fd-b621-521f50f590bf-kube-proxy") pod "kube-proxy-5gzww" (UID: "942c436a-941e-43fd-b621-521f50f590bf")
Apr 12 00:31:55 k8s61 kubelet[12675]: I0412 00:31:55.586334   12675 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "xtables-lock" (UniqueName: "kubernetes.io/host-path/942c436a-941e-43fd-b621-521f50f590bf-xtables-lock") pod "kube-proxy-5gzww" (UID: "942c436a-941e-43fd-b621-521f50f590bf")
Apr 12 00:31:55 k8s61 kubelet[12675]: I0412 00:31:55.586365   12675 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/ca2aa1b3224c37fa1791ef6c7d883bbe-kubeconfig") pod "kube-scheduler-k8s61" (UID: "ca2aa1b3224c37fa1791ef6c7d883bbe")
Apr 12 00:31:55 k8s61 kubelet[12675]: I0412 00:31:55.586492   12675 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "lib-modules" (UniqueName: "kubernetes.io/host-path/942c436a-941e-43fd-b621-521f50f590bf-lib-modules") pod "kube-proxy-5gzww" (UID: "942c436a-941e-43fd-b621-521f50f590bf")
Apr 12 00:31:55 k8s61 kubelet[12675]: I0412 00:31:55.586516   12675 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy-token-s8gfr" (UniqueName: "kubernetes.io/secret/942c436a-941e-43fd-b621-521f50f590bf-kube-proxy-token-s8gfr") pod "kube-proxy-5gzww" (UID: "942c436a-941e-43fd-b621-521f50f590bf")
Apr 12 00:31:55 k8s61 kubelet[12675]: I0412 00:31:55.686990   12675 reconciler.go:157] Reconciler: start to sync state
Apr 12 00:31:55 k8s61 kubelet[12675]: E0412 00:31:55.709853   12675 kubelet.go:1663] Failed creating a mirror pod for "kube-scheduler-k8s61_kube-system(ca2aa1b3224c37fa1791ef6c7d883bbe)": pods "kube-scheduler-k8s61" already exists
Apr 12 00:31:58 k8s61 kubelet[12675]: W0412 00:31:58.723411   12675 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
[root@k8s61 k8s]# kubectl apply -f kube
kubeadm.conf      kube-flannel.yml  
[root@k8s61 k8s]# kubectl apply -f kube-flannel.yml 
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds-amd64 created
daemonset.apps/kube-flannel-ds-arm64 created
daemonset.apps/kube-flannel-ds-arm created
daemonset.apps/kube-flannel-ds-ppc64le created
daemonset.apps/kube-flannel-ds-s390x created
[root@k8s61 k8s]# watch kubectl get pods -A -o wide
[root@k8s61 k8s]# kubectl get pods -A -o wide
NAMESPACE     NAME                            READY   STATUS              RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES
kube-system   coredns-7ff77c879f-gkvb8        0/1     ContainerCreating   0          16m   <none>           k8s61   <none>           <none>
kube-system   coredns-7ff77c879f-vvpn4        0/1     ContainerCreating   0          16m   <none>           k8s61   <none>           <none>
kube-system   etcd-k8s61                      1/1     Running             0          16m   192.168.122.61   k8s61   <none>           <none>
kube-system   kube-apiserver-k8s61            1/1     Running             0          16m   192.168.122.61   k8s61   <none>           <none>
kube-system   kube-controller-manager-k8s61   1/1     Running             0          16m   192.168.122.61   k8s61   <none>           <none>
kube-system   kube-flannel-ds-amd64-qktql     0/1     CrashLoopBackOff    2          99s   192.168.122.61   k8s61   <none>           <none>
kube-system   kube-proxy-5gzww                1/1     Running             0          16m   192.168.122.61   k8s61   <none>           <none>
kube-system   kube-scheduler-k8s61            1/1     Running             0          16m   192.168.122.61   k8s61   <none>           <none>
[root@k8s61 k8s]# kubectl describe pods kube-flannel-ds-amd64-qktql -n kube-system
Name:         kube-flannel-ds-amd64-qktql
Namespace:    kube-system
Priority:     0
Node:         k8s61/192.168.122.61
Start Time:   Sun, 12 Apr 2020 00:32:36 +0800
Labels:       app=flannel
              controller-revision-hash=77985764c
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           192.168.122.61
IPs:
  IP:           192.168.122.61
Controlled By:  DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://d93f1310d4583e5d256177474120c4a98fbf49135972464d08b2cf7a2d3c06bf
    Image:         quay.io/coreos/flannel:v0.12.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:6d451d92c921f14bfb38196aacb6e506d4593c5b3c9d40a8b8a2506010dc3e10
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 12 Apr 2020 00:33:20 +0800
      Finished:     Sun, 12 Apr 2020 00:33:21 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-kstfw (ro)
Containers:
  kube-flannel:
    Container ID:  docker://10b06ab8390e153bd0db3d2cb7132ddebcff2921ecad3dbcdff00a22d50cd296
    Image:         quay.io/coreos/flannel:v0.12.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:6d451d92c921f14bfb38196aacb6e506d4593c5b3c9d40a8b8a2506010dc3e10
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Sun, 12 Apr 2020 00:34:17 +0800
      Finished:     Sun, 12 Apr 2020 00:34:19 +0800
    Ready:          False
    Restart Count:  3
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-qktql (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-kstfw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-kstfw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-kstfw
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  119s               default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-qktql to k8s61
  Normal   Pulling    116s               kubelet, k8s61     Pulling image "quay.io/coreos/flannel:v0.12.0-amd64"
  Normal   Pulled     75s                kubelet, k8s61     Successfully pulled image "quay.io/coreos/flannel:v0.12.0-amd64"
  Normal   Created    74s                kubelet, k8s61     Created container install-cni
  Normal   Started    73s                kubelet, k8s61     Started container install-cni
  Normal   Pulled     19s (x4 over 73s)  kubelet, k8s61     Container image "quay.io/coreos/flannel:v0.12.0-amd64" already present on machine
  Normal   Created    18s (x4 over 73s)  kubelet, k8s61     Created container kube-flannel
  Normal   Started    17s (x4 over 72s)  kubelet, k8s61     Started container kube-flannel
  Warning  BackOff    1s (x5 over 66s)   kubelet, k8s61     Back-off restarting failed container
[root@k8s61 k8s]# kubectl describe pods kube-flannel-ds-amd64-qktql -n kube-system
Name:         kube-flannel-ds-amd64-qktql
Namespace:    kube-system
Priority:     0
Node:         k8s61/192.168.122.61
Start Time:   Sun, 12 Apr 2020 00:32:36 +0800
Labels:       app=flannel
              controller-revision-hash=77985764c
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           192.168.122.61
IPs:
  IP:           192.168.122.61
Controlled By:  DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://d93f1310d4583e5d256177474120c4a98fbf49135972464d08b2cf7a2d3c06bf
    Image:         quay.io/coreos/flannel:v0.12.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:6d451d92c921f14bfb38196aacb6e506d4593c5b3c9d40a8b8a2506010dc3e10
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 12 Apr 2020 00:33:20 +0800
      Finished:     Sun, 12 Apr 2020 00:33:21 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-kstfw (ro)
Containers:
  kube-flannel:
    Container ID:  docker://10b06ab8390e153bd0db3d2cb7132ddebcff2921ecad3dbcdff00a22d50cd296
    Image:         quay.io/coreos/flannel:v0.12.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:6d451d92c921f14bfb38196aacb6e506d4593c5b3c9d40a8b8a2506010dc3e10
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Sun, 12 Apr 2020 00:34:17 +0800
      Finished:     Sun, 12 Apr 2020 00:34:19 +0800
    Ready:          False
    Restart Count:  3
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-qktql (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-kstfw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-kstfw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-kstfw
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  2m16s              default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-qktql to k8s61
  Normal   Pulling    2m13s              kubelet, k8s61     Pulling image "quay.io/coreos/flannel:v0.12.0-amd64"
  Normal   Pulled     92s                kubelet, k8s61     Successfully pulled image "quay.io/coreos/flannel:v0.12.0-amd64"
  Normal   Created    91s                kubelet, k8s61     Created container install-cni
  Normal   Started    90s                kubelet, k8s61     Started container install-cni
  Normal   Pulled     36s (x4 over 90s)  kubelet, k8s61     Container image "quay.io/coreos/flannel:v0.12.0-amd64" already present on machine
  Normal   Created    35s (x4 over 90s)  kubelet, k8s61     Created container kube-flannel
  Normal   Started    34s (x4 over 89s)  kubelet, k8s61     Started container kube-flannel
  Warning  BackOff    6s (x6 over 83s)   kubelet, k8s61     Back-off restarting failed container
[root@k8s61 k8s]# kubectl describe pods kube-flannel-ds-amd64-qktql -n kube-system
Name:         kube-flannel-ds-amd64-qktql
Namespace:    kube-system
Priority:     0
Node:         k8s61/192.168.122.61
Start Time:   Sun, 12 Apr 2020 00:32:36 +0800
Labels:       app=flannel
              controller-revision-hash=77985764c
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           192.168.122.61
IPs:
  IP:           192.168.122.61
Controlled By:  DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://d93f1310d4583e5d256177474120c4a98fbf49135972464d08b2cf7a2d3c06bf
    Image:         quay.io/coreos/flannel:v0.12.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:6d451d92c921f14bfb38196aacb6e506d4593c5b3c9d40a8b8a2506010dc3e10
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 12 Apr 2020 00:33:20 +0800
      Finished:     Sun, 12 Apr 2020 00:33:21 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-kstfw (ro)
Containers:
  kube-flannel:
    Container ID:  docker://10b06ab8390e153bd0db3d2cb7132ddebcff2921ecad3dbcdff00a22d50cd296
    Image:         quay.io/coreos/flannel:v0.12.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:6d451d92c921f14bfb38196aacb6e506d4593c5b3c9d40a8b8a2506010dc3e10
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Sun, 12 Apr 2020 00:34:17 +0800
      Finished:     Sun, 12 Apr 2020 00:34:19 +0800
    Ready:          False
    Restart Count:  3
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-qktql (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-kstfw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-kstfw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-kstfw
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  2m36s               default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-qktql to k8s61
  Normal   Pulling    2m33s               kubelet, k8s61     Pulling image "quay.io/coreos/flannel:v0.12.0-amd64"
  Normal   Pulled     112s                kubelet, k8s61     Successfully pulled image "quay.io/coreos/flannel:v0.12.0-amd64"
  Normal   Created    111s                kubelet, k8s61     Created container install-cni
  Normal   Started    110s                kubelet, k8s61     Started container install-cni
  Normal   Started    54s (x4 over 109s)  kubelet, k8s61     Started container kube-flannel
  Warning  BackOff    13s (x7 over 103s)  kubelet, k8s61     Back-off restarting failed container
  Normal   Pulled     2s (x5 over 110s)   kubelet, k8s61     Container image "quay.io/coreos/flannel:v0.12.0-amd64" already present on machine
  Normal   Created    1s (x5 over 110s)   kubelet, k8s61     Created container kube-flannel
[root@k8s61 k8s]# kubectl describe pods kube-flannel-ds-amd64-qktql -n kube-system
Name:         kube-flannel-ds-amd64-qktql
Namespace:    kube-system
Priority:     0
Node:         k8s61/192.168.122.61
Start Time:   Sun, 12 Apr 2020 00:32:36 +0800
Labels:       app=flannel
              controller-revision-hash=77985764c
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           192.168.122.61
IPs:
  IP:           192.168.122.61
Controlled By:  DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://d93f1310d4583e5d256177474120c4a98fbf49135972464d08b2cf7a2d3c06bf
    Image:         quay.io/coreos/flannel:v0.12.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:6d451d92c921f14bfb38196aacb6e506d4593c5b3c9d40a8b8a2506010dc3e10
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 12 Apr 2020 00:33:20 +0800
      Finished:     Sun, 12 Apr 2020 00:33:21 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-kstfw (ro)
Containers:
  kube-flannel:
    Container ID:  docker://91d37b9045b14c035ec281aee074c4c1714d6041ee344208b2b98b36611c3a21
    Image:         quay.io/coreos/flannel:v0.12.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:6d451d92c921f14bfb38196aacb6e506d4593c5b3c9d40a8b8a2506010dc3e10
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Sun, 12 Apr 2020 00:35:10 +0800
      Finished:     Sun, 12 Apr 2020 00:35:12 +0800
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Sun, 12 Apr 2020 00:34:17 +0800
      Finished:     Sun, 12 Apr 2020 00:34:19 +0800
    Ready:          False
    Restart Count:  4
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-qktql (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-kstfw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-kstfw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-kstfw
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  2m47s               default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-qktql to k8s61
  Normal   Pulling    2m44s               kubelet, k8s61     Pulling image "quay.io/coreos/flannel:v0.12.0-amd64"
  Normal   Pulled     2m3s                kubelet, k8s61     Successfully pulled image "quay.io/coreos/flannel:v0.12.0-amd64"
  Normal   Created    2m2s                kubelet, k8s61     Created container install-cni
  Normal   Started    2m1s                kubelet, k8s61     Started container install-cni
  Normal   Started    65s (x4 over 2m)    kubelet, k8s61     Started container kube-flannel
  Warning  BackOff    24s (x7 over 114s)  kubelet, k8s61     Back-off restarting failed container
  Normal   Pulled     13s (x5 over 2m1s)  kubelet, k8s61     Container image "quay.io/coreos/flannel:v0.12.0-amd64" already present on machine
  Normal   Created    12s (x5 over 2m1s)  kubelet, k8s61     Created container kube-flannel
[root@k8s61 k8s]# kubectl describe pods kube-flannel-ds-amd64-qktql -n kube-system
Name:         kube-flannel-ds-amd64-qktql
Namespace:    kube-system
Priority:     0
Node:         k8s61/192.168.122.61
Start Time:   Sun, 12 Apr 2020 00:32:36 +0800
Labels:       app=flannel
              controller-revision-hash=77985764c
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           192.168.122.61
IPs:
  IP:           192.168.122.61
Controlled By:  DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://d93f1310d4583e5d256177474120c4a98fbf49135972464d08b2cf7a2d3c06bf
    Image:         quay.io/coreos/flannel:v0.12.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:6d451d92c921f14bfb38196aacb6e506d4593c5b3c9d40a8b8a2506010dc3e10
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 12 Apr 2020 00:33:20 +0800
      Finished:     Sun, 12 Apr 2020 00:33:21 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-kstfw (ro)
Containers:
  kube-flannel:
    Container ID:  docker://91d37b9045b14c035ec281aee074c4c1714d6041ee344208b2b98b36611c3a21
    Image:         quay.io/coreos/flannel:v0.12.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:6d451d92c921f14bfb38196aacb6e506d4593c5b3c9d40a8b8a2506010dc3e10
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Sun, 12 Apr 2020 00:35:10 +0800
      Finished:     Sun, 12 Apr 2020 00:35:12 +0800
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Sun, 12 Apr 2020 00:34:17 +0800
      Finished:     Sun, 12 Apr 2020 00:34:19 +0800
    Ready:          False
    Restart Count:  4
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-qktql (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-kstfw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-kstfw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-kstfw
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  2m49s               default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-qktql to k8s61
  Normal   Pulling    2m46s               kubelet, k8s61     Pulling image "quay.io/coreos/flannel:v0.12.0-amd64"
  Normal   Pulled     2m5s                kubelet, k8s61     Successfully pulled image "quay.io/coreos/flannel:v0.12.0-amd64"
  Normal   Created    2m4s                kubelet, k8s61     Created container install-cni
  Normal   Started    2m3s                kubelet, k8s61     Started container install-cni
  Normal   Started    67s (x4 over 2m2s)  kubelet, k8s61     Started container kube-flannel
  Warning  BackOff    26s (x7 over 116s)  kubelet, k8s61     Back-off restarting failed container
  Normal   Pulled     15s (x5 over 2m3s)  kubelet, k8s61     Container image "quay.io/coreos/flannel:v0.12.0-amd64" already present on machine
  Normal   Created    14s (x5 over 2m3s)  kubelet, k8s61     Created container kube-flannel
[root@k8s61 k8s]# kubectl describe pods kube-flannel-ds-amd64-qktql -n kube-system
Name:         kube-flannel-ds-amd64-qktql
Namespace:    kube-system
Priority:     0
Node:         k8s61/192.168.122.61
Start Time:   Sun, 12 Apr 2020 00:32:36 +0800
Labels:       app=flannel
              controller-revision-hash=77985764c
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           192.168.122.61
IPs:
  IP:           192.168.122.61
Controlled By:  DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://d93f1310d4583e5d256177474120c4a98fbf49135972464d08b2cf7a2d3c06bf
    Image:         quay.io/coreos/flannel:v0.12.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:6d451d92c921f14bfb38196aacb6e506d4593c5b3c9d40a8b8a2506010dc3e10
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 12 Apr 2020 00:33:20 +0800
      Finished:     Sun, 12 Apr 2020 00:33:21 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-kstfw (ro)
Containers:
  kube-flannel:
    Container ID:  docker://91d37b9045b14c035ec281aee074c4c1714d6041ee344208b2b98b36611c3a21
    Image:         quay.io/coreos/flannel:v0.12.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:6d451d92c921f14bfb38196aacb6e506d4593c5b3c9d40a8b8a2506010dc3e10
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Sun, 12 Apr 2020 00:35:10 +0800
      Finished:     Sun, 12 Apr 2020 00:35:12 +0800
    Ready:          False
    Restart Count:  4
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-qktql (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-kstfw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-kstfw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-kstfw
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  3m22s                 default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-qktql to k8s61
  Normal   Pulling    3m19s                 kubelet, k8s61     Pulling image "quay.io/coreos/flannel:v0.12.0-amd64"
  Normal   Pulled     2m38s                 kubelet, k8s61     Successfully pulled image "quay.io/coreos/flannel:v0.12.0-amd64"
  Normal   Created    2m37s                 kubelet, k8s61     Created container install-cni
  Normal   Started    2m36s                 kubelet, k8s61     Started container install-cni
  Normal   Started    100s (x4 over 2m35s)  kubelet, k8s61     Started container kube-flannel
  Warning  BackOff    59s (x7 over 2m29s)   kubelet, k8s61     Back-off restarting failed container
  Normal   Pulled     48s (x5 over 2m36s)   kubelet, k8s61     Container image "quay.io/coreos/flannel:v0.12.0-amd64" already present on machine
  Normal   Created    47s (x5 over 2m36s)   kubelet, k8s61     Created container kube-flannel
[root@k8s61 k8s]# kubectl get pods -A -o wide
NAMESPACE     NAME                            READY   STATUS              RESTARTS   AGE     IP               NODE    NOMINATED NODE   READINESS GATES
kube-system   coredns-7ff77c879f-gkvb8        0/1     ContainerCreating   0          18m     <none>           k8s61   <none>           <none>
kube-system   coredns-7ff77c879f-vvpn4        0/1     ContainerCreating   0          18m     <none>           k8s61   <none>           <none>
kube-system   etcd-k8s61                      1/1     Running             0          18m     192.168.122.61   k8s61   <none>           <none>
kube-system   kube-apiserver-k8s61            1/1     Running             0          18m     192.168.122.61   k8s61   <none>           <none>
kube-system   kube-controller-manager-k8s61   1/1     Running             0          18m     192.168.122.61   k8s61   <none>           <none>
kube-system   kube-flannel-ds-amd64-qktql     0/1     CrashLoopBackOff    4          3m26s   192.168.122.61   k8s61   <none>           <none>
kube-system   kube-proxy-5gzww                1/1     Running             0          18m     192.168.122.61   k8s61   <none>           <none>
kube-system   kube-scheduler-k8s61            1/1     Running             0          18m     192.168.122.61   k8s61   <none>           <none>
[root@k8s61 k8s]# kubectl get pods -A -o wide
NAMESPACE     NAME                            READY   STATUS              RESTARTS   AGE    IP               NODE    NOMINATED NODE   READINESS GATES
kube-system   coredns-7ff77c879f-gkvb8        0/1     ContainerCreating   0          19m    <none>           k8s61   <none>           <none>
kube-system   coredns-7ff77c879f-vvpn4        0/1     ContainerCreating   0          19m    <none>           k8s61   <none>           <none>
kube-system   etcd-k8s61                      1/1     Running             0          18m    192.168.122.61   k8s61   <none>           <none>
kube-system   kube-apiserver-k8s61            1/1     Running             0          18m    192.168.122.61   k8s61   <none>           <none>
kube-system   kube-controller-manager-k8s61   1/1     Running             0          18m    192.168.122.61   k8s61   <none>           <none>
kube-system   kube-flannel-ds-amd64-qktql     0/1     CrashLoopBackOff    4          4m7s   192.168.122.61   k8s61   <none>           <none>
kube-system   kube-proxy-5gzww                1/1     Running             0          19m    192.168.122.61   k8s61   <none>           <none>
kube-system   kube-scheduler-k8s61            1/1     Running             0          18m    192.168.122.61   k8s61   <none>           <none>
[root@k8s61 k8s]# kubectl delete -f kube-flannel.yml 
podsecuritypolicy.policy "psp.flannel.unprivileged" deleted
clusterrole.rbac.authorization.k8s.io "flannel" deleted
clusterrolebinding.rbac.authorization.k8s.io "flannel" deleted
serviceaccount "flannel" deleted
configmap "kube-flannel-cfg" deleted
daemonset.apps "kube-flannel-ds-amd64" deleted
daemonset.apps "kube-flannel-ds-arm64" deleted
daemonset.apps "kube-flannel-ds-arm" deleted
daemonset.apps "kube-flannel-ds-ppc64le" deleted
daemonset.apps "kube-flannel-ds-s390x" deleted
[root@k8s61 k8s]# rm -f /etc/cni/net.d/*
[root@k8s61 k8s]# rm -rf /var/lib/cni/
[root@k8s61 k8s]# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:a6:f9:de brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.61/24 brd 192.168.122.255 scope global noprefixroute eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::5054:ff:fea6:f9de/64 scope link 
       valid_lft forever preferred_lft forever
3: enp2s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:32:bb:f0 brd ff:ff:ff:ff:ff:ff
    inet 192.168.100.61/24 brd 192.168.100.255 scope global noprefixroute enp2s0
       valid_lft forever preferred_lft forever
    inet6 240e:f8:a900:a5b6:5054:ff:fe32:bbf0/64 scope global mngtmpaddr dynamic 
       valid_lft 7111sec preferred_lft 7111sec
    inet6 fe80::5054:ff:fe32:bbf0/64 scope link 
       valid_lft forever preferred_lft forever
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:9a:4d:be:5b brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: dummy0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether e2:1e:c2:4c:d6:46 brd ff:ff:ff:ff:ff:ff
6: kube-ipvs0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default 
    link/ether 46:79:73:ee:64:c7 brd ff:ff:ff:ff:ff:ff
    inet 10.244.0.10/32 brd 10.244.0.10 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.244.0.1/32 brd 10.244.0.1 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
7: flannel.1: <BROADCAST,MULTICAST> mtu 1450 qdisc noop state DOWN group default 
    link/ether 82:dd:e9:32:09:45 brd ff:ff:ff:ff:ff:ff
[root@k8s61 k8s]# ifconfig flannel.1 down
[root@k8s61 k8s]# ip link delete flannel.1
[root@k8s61 k8s]# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:a6:f9:de brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.61/24 brd 192.168.122.255 scope global noprefixroute eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::5054:ff:fea6:f9de/64 scope link 
       valid_lft forever preferred_lft forever
3: enp2s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:32:bb:f0 brd ff:ff:ff:ff:ff:ff
    inet 192.168.100.61/24 brd 192.168.100.255 scope global noprefixroute enp2s0
       valid_lft forever preferred_lft forever
    inet6 240e:f8:a900:a5b6:5054:ff:fe32:bbf0/64 scope global mngtmpaddr dynamic 
       valid_lft 7166sec preferred_lft 7166sec
    inet6 fe80::5054:ff:fe32:bbf0/64 scope link 
       valid_lft forever preferred_lft forever
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:9a:4d:be:5b brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: dummy0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether e2:1e:c2:4c:d6:46 brd ff:ff:ff:ff:ff:ff
6: kube-ipvs0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default 
    link/ether 46:79:73:ee:64:c7 brd ff:ff:ff:ff:ff:ff
    inet 10.244.0.10/32 brd 10.244.0.10 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.244.0.1/32 brd 10.244.0.1 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
[root@k8s61 k8s]# init 6
Last login: Sun Apr 12 00:12:44 2020 from 192.168.100.115
[root@k8s61 ~]# wget https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
--2020-04-12 01:16:06--  https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
Resolving docs.projectcalico.org (docs.projectcalico.org)... 2400:6180:0:d1::4d8:4001, 206.189.89.118
Connecting to docs.projectcalico.org (docs.projectcalico.org)|2400:6180:0:d1::4d8:4001|:443... failed: Connection timed out.
Connecting to docs.projectcalico.org (docs.projectcalico.org)|206.189.89.118|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 20679 (20K) [application/x-yaml]
Saving to: ‘calico.yaml’

100%[=========================================================================================================================================>] 20,679      13.9KB/s   in 1.5s   

2020-04-12 01:18:17 (13.9 KB/s) - ‘calico.yaml’ saved [20679/20679]

[root@k8s61 ~]# vim calico.yaml 
[root@k8s61 ~]# kubectl apply -f calico.yaml 
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
[root@k8s61 ~]# cd k8s/
[root@k8s61 k8s]# ll
total 44
-rw-r--r-- 1 root root 21075 Apr 12 00:16 calico.yaml
-rw-r--r-- 1 root root   759 Apr 12 00:14 kubeadm.conf
-rw-r--r-- 1 root root 14439 Apr 12 00:31 kube-flannel.yml
[root@k8s61 k8s]# rm calico.yaml 
rm: remove regular file ‘calico.yaml’? y
[root@k8s61 k8s]# mv ../calico.yaml ./
[root@k8s61 k8s]# ll
total 44
-rw-r--r-- 1 root root 20678 Apr 12 01:18 calico.yaml
-rw-r--r-- 1 root root   759 Apr 12 00:14 kubeadm.conf
-rw-r--r-- 1 root root 14439 Apr 12 00:31 kube-flannel.yml
[root@k8s61 k8s]# kubectl get pods
pods                        podsecuritypolicies.policy  
[root@k8s61 k8s]# kubectl get pods
pods                        podsecuritypolicies.policy  
[root@k8s61 k8s]# kubectl get pods -A -o wide
NAMESPACE     NAME                                      READY   STATUS              RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
kube-system   calico-kube-controllers-dc4469c7f-tgnmm   0/1     Pending             0          39s   <none>           <none>   <none>           <none>
kube-system   calico-node-lf24x                         0/1     Init:0/3            0          39s   192.168.122.61   k8s61    <none>           <none>
kube-system   coredns-7ff77c879f-gkvb8                  0/1     ContainerCreating   0          61m   <none>           k8s61    <none>           <none>
kube-system   coredns-7ff77c879f-vvpn4                  0/1     ContainerCreating   0          61m   <none>           k8s61    <none>           <none>
kube-system   etcd-k8s61                                1/1     Running             1          61m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-apiserver-k8s61                      1/1     Running             1          61m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-controller-manager-k8s61             1/1     Running             1          61m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-proxy-5gzww                          1/1     Running             1          61m   192.168.122.61   k8s61    <none>           <none>
kube-system   kube-scheduler-k8s61                      1/1     Running             1          61m   192.168.122.61   k8s61    <none>           <none>
[root@k8s61 k8s]# watch kubectl get pods -A -o wide
[root@k8s61 k8s]# kubectl describe pods calico-node-lf24x -n kube-system 
Name:                 calico-node-lf24x
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 k8s61/192.168.122.61
Start Time:           Sun, 12 Apr 2020 01:18:57 +0800
Labels:               controller-revision-hash=7f8c6f6645
                      k8s-app=calico-node
                      pod-template-generation=1
Annotations:          scheduler.alpha.kubernetes.io/critical-pod: 
Status:               Pending
IP:                   192.168.122.61
IPs:
  IP:           192.168.122.61
Controlled By:  DaemonSet/calico-node
Init Containers:
  upgrade-ipam:
    Container ID:  
    Image:         calico/cni:v3.10.3
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/cni/bin/calico-ipam
      -upgrade
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      KUBERNETES_NODE_NAME:        (v1:spec.nodeName)
      CALICO_NETWORKING_BACKEND:  <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
    Mounts:
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/lib/cni/networks from host-local-net-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
  install-cni:
    Container ID:  
    Image:         calico/cni:v3.10.3
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /install-cni.sh
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
      CNI_MTU:               <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false
      SLEEP:                 false
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
  flexvol-driver:
    Container ID:   
    Image:          calico/pod2daemon-flexvol:v3.10.3
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /host/driver from flexvol-driver-host (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
Containers:
  calico-node:
    Container ID:   
    Image:          calico/node:v3.10.3
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:      250m
    Liveness:   exec [/bin/calico-node -felix-live -bird-live] delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  exec [/bin/calico-node -felix-ready -bird-ready] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      WAIT_FOR_DATASTORE:                 true
      NODENAME:                            (v1:spec.nodeName)
      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
      CLUSTER_TYPE:                       k8s,bgp
      IP:                                 autodetect
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPMTU:                    <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false
      CALICO_IPV4POOL_CIDR:               10.244.0.0/16
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_LOGSEVERITYSCREEN:            info
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/calico from var-lib-calico (rw)
      /var/run/calico from var-run-calico (rw)
      /var/run/nodeagent from policysync (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
Conditions:
  Type              Status
  Initialized       False 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  var-run-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/calico
    HostPathType:  
  var-lib-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/calico
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  cni-bin-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/cni/bin
    HostPathType:  
  cni-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  host-local-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/networks
    HostPathType:  
  policysync:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/nodeagent
    HostPathType:  DirectoryOrCreate
  flexvol-driver-host:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
    HostPathType:  DirectoryOrCreate
  calico-node-token-7plqb:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-node-token-7plqb
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     :NoSchedule
                 :NoExecute
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  2m36s  default-scheduler  Successfully assigned kube-system/calico-node-lf24x to k8s61
  Normal  Pulling    2m33s  kubelet, k8s61     Pulling image "calico/cni:v3.10.3"
[root@k8s61 k8s]# kubectl describe pods calico-node-lf24x -n kube-system 
Name:                 calico-node-lf24x
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 k8s61/192.168.122.61
Start Time:           Sun, 12 Apr 2020 01:18:57 +0800
Labels:               controller-revision-hash=7f8c6f6645
                      k8s-app=calico-node
                      pod-template-generation=1
Annotations:          scheduler.alpha.kubernetes.io/critical-pod: 
Status:               Pending
IP:                   192.168.122.61
IPs:
  IP:           192.168.122.61
Controlled By:  DaemonSet/calico-node
Init Containers:
  upgrade-ipam:
    Container ID:  
    Image:         calico/cni:v3.10.3
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/cni/bin/calico-ipam
      -upgrade
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      KUBERNETES_NODE_NAME:        (v1:spec.nodeName)
      CALICO_NETWORKING_BACKEND:  <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
    Mounts:
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/lib/cni/networks from host-local-net-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
  install-cni:
    Container ID:  
    Image:         calico/cni:v3.10.3
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /install-cni.sh
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
      CNI_MTU:               <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false
      SLEEP:                 false
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
  flexvol-driver:
    Container ID:   
    Image:          calico/pod2daemon-flexvol:v3.10.3
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /host/driver from flexvol-driver-host (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
Containers:
  calico-node:
    Container ID:   
    Image:          calico/node:v3.10.3
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:      250m
    Liveness:   exec [/bin/calico-node -felix-live -bird-live] delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  exec [/bin/calico-node -felix-ready -bird-ready] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      WAIT_FOR_DATASTORE:                 true
      NODENAME:                            (v1:spec.nodeName)
      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
      CLUSTER_TYPE:                       k8s,bgp
      IP:                                 autodetect
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPMTU:                    <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false
      CALICO_IPV4POOL_CIDR:               10.244.0.0/16
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_LOGSEVERITYSCREEN:            info
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/calico from var-lib-calico (rw)
      /var/run/calico from var-run-calico (rw)
      /var/run/nodeagent from policysync (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
Conditions:
  Type              Status
  Initialized       False 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  var-run-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/calico
    HostPathType:  
  var-lib-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/calico
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  cni-bin-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/cni/bin
    HostPathType:  
  cni-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  host-local-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/networks
    HostPathType:  
  policysync:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/nodeagent
    HostPathType:  DirectoryOrCreate
  flexvol-driver-host:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
    HostPathType:  DirectoryOrCreate
  calico-node-token-7plqb:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-node-token-7plqb
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     :NoSchedule
                 :NoExecute
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  3m35s  default-scheduler  Successfully assigned kube-system/calico-node-lf24x to k8s61
  Normal  Pulling    3m32s  kubelet, k8s61     Pulling image "calico/cni:v3.10.3"
[root@k8s61 k8s]# init 0
Last login: Sun Apr 12 01:16:01 2020 from 192.168.100.115
[root@k8s61 ~]# vim /etc/sys
sysconfig/          sysctl.conf         sysctl.d/           systemd/            system-release      system-release-cpe  
[root@k8s61 ~]# vim /etc/sysconfig/network-scripts/ifcfg-
ifcfg-enp2s0  ifcfg-eth0    ifcfg-lo      
[root@k8s61 ~]# vim /etc/sysconfig/network-scripts/ifcfg-
ifcfg-enp2s0  ifcfg-eth0    ifcfg-lo      
[root@k8s61 ~]# vim /etc/sysconfig/network-scripts/ifcfg-enp2s0 
[root@k8s61 ~]# init 6
Last login: Sun Apr 12 01:29:01 2020
[root@k8s61 ~]# kubectl describe pods calico-node-lf24x -n kube-system 
Name:                 calico-node-lf24x
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 k8s61/192.168.122.61
Start Time:           Sun, 12 Apr 2020 01:18:57 +0800
Labels:               controller-revision-hash=7f8c6f6645
                      k8s-app=calico-node
                      pod-template-generation=1
Annotations:          scheduler.alpha.kubernetes.io/critical-pod: 
Status:               Pending
IP:                   192.168.122.61
IPs:
  IP:           192.168.122.61
Controlled By:  DaemonSet/calico-node
Init Containers:
  upgrade-ipam:
    Container ID:  docker://358c3f7545245fc1681ae8af7a502bed8a1e6f408e5c1d6827a9605084667b15
    Image:         calico/cni:v3.10.3
    Image ID:      docker-pullable://calico/cni@sha256:dd9840a37c296f42da6affc4c2e5285af772536c58f5eab0feafac9a1fceb48d
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/cni/bin/calico-ipam
      -upgrade
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 12 Apr 2020 01:29:18 +0800
      Finished:     Sun, 12 Apr 2020 01:29:18 +0800
    Ready:          True
    Restart Count:  1
    Environment:
      KUBERNETES_NODE_NAME:        (v1:spec.nodeName)
      CALICO_NETWORKING_BACKEND:  <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
    Mounts:
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/lib/cni/networks from host-local-net-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
  install-cni:
    Container ID:  docker://aedc39cae02d5159e395fa3c7a84e104fe087f2bce32c52c06ea4e00519f3366
    Image:         calico/cni:v3.10.3
    Image ID:      docker-pullable://calico/cni@sha256:dd9840a37c296f42da6affc4c2e5285af772536c58f5eab0feafac9a1fceb48d
    Port:          <none>
    Host Port:     <none>
    Command:
      /install-cni.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 12 Apr 2020 01:29:22 +0800
      Finished:     Sun, 12 Apr 2020 01:29:22 +0800
    Ready:          True
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
      CNI_MTU:               <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false
      SLEEP:                 false
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
  flexvol-driver:
    Container ID:   docker://548afb25c1ac74c2f7f893f643b07708a9190cd2671b13055d7123aae8dbf9dd
    Image:          calico/pod2daemon-flexvol:v3.10.3
    Image ID:       docker-pullable://calico/pod2daemon-flexvol@sha256:f23c709f991553b75a9a8c6f156c4f61f47097424d6e5b0e6e9319da98a86185
    Port:           <none>
    Host Port:      <none>
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 12 Apr 2020 01:29:27 +0800
      Finished:     Sun, 12 Apr 2020 01:29:27 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /host/driver from flexvol-driver-host (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
Containers:
  calico-node:
    Container ID:   
    Image:          calico/node:v3.10.3
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:      250m
    Liveness:   exec [/bin/calico-node -felix-live -bird-live] delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  exec [/bin/calico-node -felix-ready -bird-ready] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      WAIT_FOR_DATASTORE:                 true
      NODENAME:                            (v1:spec.nodeName)
      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
      CLUSTER_TYPE:                       k8s,bgp
      IP:                                 autodetect
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPMTU:                    <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false
      CALICO_IPV4POOL_CIDR:               10.244.0.0/16
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_LOGSEVERITYSCREEN:            info
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/calico from var-lib-calico (rw)
      /var/run/calico from var-run-calico (rw)
      /var/run/nodeagent from policysync (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  var-run-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/calico
    HostPathType:  
  var-lib-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/calico
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  cni-bin-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/cni/bin
    HostPathType:  
  cni-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  host-local-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/networks
    HostPathType:  
  policysync:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/nodeagent
    HostPathType:  DirectoryOrCreate
  flexvol-driver-host:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
    HostPathType:  DirectoryOrCreate
  calico-node-token-7plqb:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-node-token-7plqb
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     :NoSchedule
                 :NoExecute
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason          Age    From               Message
  ----    ------          ----   ----               -------
  Normal  Scheduled       10m    default-scheduler  Successfully assigned kube-system/calico-node-lf24x to k8s61
  Normal  Pulling         10m    kubelet, k8s61     Pulling image "calico/cni:v3.10.3"
  Normal  Pulling         3m39s  kubelet, k8s61     Pulling image "calico/cni:v3.10.3"
  Normal  Pulled          3m17s  kubelet, k8s61     Successfully pulled image "calico/cni:v3.10.3"
  Normal  Created         3m14s  kubelet, k8s61     Created container upgrade-ipam
  Normal  Started         3m13s  kubelet, k8s61     Started container upgrade-ipam
  Normal  Pulled          3m13s  kubelet, k8s61     Container image "calico/cni:v3.10.3" already present on machine
  Normal  Created         3m12s  kubelet, k8s61     Created container install-cni
  Normal  Started         3m12s  kubelet, k8s61     Started container install-cni
  Normal  Pulling         3m10s  kubelet, k8s61     Pulling image "calico/pod2daemon-flexvol:v3.10.3"
  Normal  Pulled          2m53s  kubelet, k8s61     Successfully pulled image "calico/pod2daemon-flexvol:v3.10.3"
  Normal  Created         2m52s  kubelet, k8s61     Created container flexvol-driver
  Normal  Started         2m50s  kubelet, k8s61     Started container flexvol-driver
  Normal  SandboxChanged  46s    kubelet, k8s61     Pod sandbox changed, it will be killed and re-created.
  Normal  Pulled          39s    kubelet, k8s61     Container image "calico/cni:v3.10.3" already present on machine
  Normal  Created         38s    kubelet, k8s61     Created container upgrade-ipam
  Normal  Started         37s    kubelet, k8s61     Started container upgrade-ipam
  Normal  Pulled          35s    kubelet, k8s61     Container image "calico/cni:v3.10.3" already present on machine
  Normal  Created         34s    kubelet, k8s61     Created container install-cni
  Normal  Started         31s    kubelet, k8s61     Started container install-cni
  Normal  Pulled          30s    kubelet, k8s61     Container image "calico/pod2daemon-flexvol:v3.10.3" already present on machine
  Normal  Created         29s    kubelet, k8s61     Created container flexvol-driver
  Normal  Started         27s    kubelet, k8s61     Started container flexvol-driver
  Normal  Pulling         27s    kubelet, k8s61     Pulling image "calico/node:v3.10.3"
[root@k8s61 ~]# watch kubectl get pods -A -o wide
[root@k8s61 ~]# 
[root@k8s61 k8s]# init 0
Last login: Sun Apr 12 01:16:01 2020 from 192.168.100.115
[root@k8s61 ~]# vim /etc/sys
sysconfig/          sysctl.conf         sysctl.d/           systemd/            system-release      system-release-cpe  
[root@k8s61 ~]# vim /etc/sysconfig/network-scripts/ifcfg-
ifcfg-enp2s0  ifcfg-eth0    ifcfg-lo      
[root@k8s61 ~]# vim /etc/sysconfig/network-scripts/ifcfg-
ifcfg-enp2s0  ifcfg-eth0    ifcfg-lo      
[root@k8s61 ~]# vim /etc/sysconfig/network-scripts/ifcfg-enp2s0 
[root@k8s61 ~]# init 6
Last login: Sun Apr 12 01:29:01 2020
[root@k8s61 ~]# kubectl describe pods calico-node-lf24x -n kube-system 
Name:                 calico-node-lf24x
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 k8s61/192.168.122.61
Start Time:           Sun, 12 Apr 2020 01:18:57 +0800
Labels:               controller-revision-hash=7f8c6f6645
                      k8s-app=calico-node
                      pod-template-generation=1
Annotations:          scheduler.alpha.kubernetes.io/critical-pod: 
Status:               Pending
IP:                   192.168.122.61
IPs:
  IP:           192.168.122.61
Controlled By:  DaemonSet/calico-node
Init Containers:
  upgrade-ipam:
    Container ID:  docker://358c3f7545245fc1681ae8af7a502bed8a1e6f408e5c1d6827a9605084667b15
    Image:         calico/cni:v3.10.3
    Image ID:      docker-pullable://calico/cni@sha256:dd9840a37c296f42da6affc4c2e5285af772536c58f5eab0feafac9a1fceb48d
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/cni/bin/calico-ipam
      -upgrade
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 12 Apr 2020 01:29:18 +0800
      Finished:     Sun, 12 Apr 2020 01:29:18 +0800
    Ready:          True
    Restart Count:  1
    Environment:
      KUBERNETES_NODE_NAME:        (v1:spec.nodeName)
      CALICO_NETWORKING_BACKEND:  <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
    Mounts:
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/lib/cni/networks from host-local-net-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
  install-cni:
    Container ID:  docker://aedc39cae02d5159e395fa3c7a84e104fe087f2bce32c52c06ea4e00519f3366
    Image:         calico/cni:v3.10.3
    Image ID:      docker-pullable://calico/cni@sha256:dd9840a37c296f42da6affc4c2e5285af772536c58f5eab0feafac9a1fceb48d
    Port:          <none>
    Host Port:     <none>
    Command:
      /install-cni.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 12 Apr 2020 01:29:22 +0800
      Finished:     Sun, 12 Apr 2020 01:29:22 +0800
    Ready:          True
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
      CNI_MTU:               <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false
      SLEEP:                 false
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
  flexvol-driver:
    Container ID:   docker://548afb25c1ac74c2f7f893f643b07708a9190cd2671b13055d7123aae8dbf9dd
    Image:          calico/pod2daemon-flexvol:v3.10.3
    Image ID:       docker-pullable://calico/pod2daemon-flexvol@sha256:f23c709f991553b75a9a8c6f156c4f61f47097424d6e5b0e6e9319da98a86185
    Port:           <none>
    Host Port:      <none>
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 12 Apr 2020 01:29:27 +0800
      Finished:     Sun, 12 Apr 2020 01:29:27 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /host/driver from flexvol-driver-host (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
Containers:
  calico-node:
    Container ID:   
    Image:          calico/node:v3.10.3
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:      250m
    Liveness:   exec [/bin/calico-node -felix-live -bird-live] delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  exec [/bin/calico-node -felix-ready -bird-ready] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      WAIT_FOR_DATASTORE:                 true
      NODENAME:                            (v1:spec.nodeName)
      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
      CLUSTER_TYPE:                       k8s,bgp
      IP:                                 autodetect
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPMTU:                    <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false
      CALICO_IPV4POOL_CIDR:               10.244.0.0/16
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_LOGSEVERITYSCREEN:            info
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/calico from var-lib-calico (rw)
      /var/run/calico from var-run-calico (rw)
      /var/run/nodeagent from policysync (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-7plqb (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  var-run-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/calico
    HostPathType:  
  var-lib-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/calico
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  cni-bin-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/cni/bin
    HostPathType:  
  cni-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  host-local-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/networks
    HostPathType:  
  policysync:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/nodeagent
    HostPathType:  DirectoryOrCreate
  flexvol-driver-host:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
    HostPathType:  DirectoryOrCreate
  calico-node-token-7plqb:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-node-token-7plqb
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     :NoSchedule
                 :NoExecute
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason          Age    From               Message
  ----    ------          ----   ----               -------
  Normal  Scheduled       10m    default-scheduler  Successfully assigned kube-system/calico-node-lf24x to k8s61
  Normal  Pulling         10m    kubelet, k8s61     Pulling image "calico/cni:v3.10.3"
  Normal  Pulling         3m39s  kubelet, k8s61     Pulling image "calico/cni:v3.10.3"
  Normal  Pulled          3m17s  kubelet, k8s61     Successfully pulled image "calico/cni:v3.10.3"
  Normal  Created         3m14s  kubelet, k8s61     Created container upgrade-ipam
  Normal  Started         3m13s  kubelet, k8s61     Started container upgrade-ipam
  Normal  Pulled          3m13s  kubelet, k8s61     Container image "calico/cni:v3.10.3" already present on machine
  Normal  Created         3m12s  kubelet, k8s61     Created container install-cni
  Normal  Started         3m12s  kubelet, k8s61     Started container install-cni
  Normal  Pulling         3m10s  kubelet, k8s61     Pulling image "calico/pod2daemon-flexvol:v3.10.3"
  Normal  Pulled          2m53s  kubelet, k8s61     Successfully pulled image "calico/pod2daemon-flexvol:v3.10.3"
  Normal  Created         2m52s  kubelet, k8s61     Created container flexvol-driver
  Normal  Started         2m50s  kubelet, k8s61     Started container flexvol-driver
  Normal  SandboxChanged  46s    kubelet, k8s61     Pod sandbox changed, it will be killed and re-created.
  Normal  Pulled          39s    kubelet, k8s61     Container image "calico/cni:v3.10.3" already present on machine
  Normal  Created         38s    kubelet, k8s61     Created container upgrade-ipam
  Normal  Started         37s    kubelet, k8s61     Started container upgrade-ipam
  Normal  Pulled          35s    kubelet, k8s61     Container image "calico/cni:v3.10.3" already present on machine
  Normal  Created         34s    kubelet, k8s61     Created container install-cni
  Normal  Started         31s    kubelet, k8s61     Started container install-cni
  Normal  Pulled          30s    kubelet, k8s61     Container image "calico/pod2daemon-flexvol:v3.10.3" already present on machine
  Normal  Created         29s    kubelet, k8s61     Created container flexvol-driver
  Normal  Started         27s    kubelet, k8s61     Started container flexvol-driver
  Normal  Pulling         27s    kubelet, k8s61     Pulling image "calico/node:v3.10.3"
[root@k8s61 ~]# watch kubectl get pods -A -o wide
[root@k8s61 ~]# watch kubectl get pods -A -o wide
[root@k8s61 ~]# 

[root@k8s61 k8s]# ll
total 44
-rw-r--r-- 1 root root 20678 Apr 12 01:18 calico.yaml
-rw-r--r-- 1 root root   759 Apr 12 00:14 kubeadm.conf
-rw-r--r-- 1 root root 14439 Apr 12 00:31 kube-flannel.yml
drwxr-xr-x 2 root root   201 Apr 12 01:41 kubernetes-dashboard2.0.0-deploy
[root@k8s61 k8s]# cd kubernetes-dashboard2.0.0-deploy/
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# ll
total 24
-rw-r--r-- 1 root root  645 Apr 12 01:41 k8s-dashboard-configmap-secret.yaml
-rw-r--r-- 1 root root 2204 Apr 12 01:41 k8s-dashboard-deploy.yaml
-rw-r--r-- 1 root root 1913 Apr 12 01:41 k8s-dashboard-rbac.yaml
-rw-r--r-- 1 root root  515 Apr 12 01:41 k8s-dashboard-token.yaml
-rw-r--r-- 1 root root 2027 Apr 12 01:41 kubernetes-metrics-scraper.yaml
-rw-r--r-- 1 root root  261 Apr 12 01:41 README.md
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# rm README.md 
rm: remove regular file ‘README.md’? y
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# kubectl apply -f k8s-dashboard-rbac.yaml 
serviceaccount/kubernetes-dashboard created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# kubectl apply -f k8s-dashboard-configmap-secret.yaml 
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# kubectl apply -f k8s-dashboard-deploy.yaml 
service/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# kubectl apply -f kubernetes-metrics-scraper.yaml 
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# kubectl apply -f k8s-dashboard-token.yaml 
clusterrolebinding.rbac.authorization.k8s.io/tinychen created
serviceaccount/admin created
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# vim k8s-dashboard-token.yaml 
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# kubectl delete -f k8s-dashboard-token.yaml 
clusterrolebinding.rbac.authorization.k8s.io "tinychen" deleted
serviceaccount "admin" deleted
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# vim k8s-dashboard-token.yaml 
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# kubectl apply -f k8s-dashboard-token.yaml 
clusterrolebinding.rbac.authorization.k8s.io/tinychen created
serviceaccount/tinychen created
[root@k8s61 kubernetes-dashboard2.0.0-deploy]#  kubectl describe secret/$(kubectl get secret -n kube-system |grep tinychen|awk '{print $1}') -n kube-system
Name:         tinychen-token-6w956
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: tinychen
              kubernetes.io/service-account.uid: fc01ac14-82d3-4fd9-86d4-50adca044175

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IlpfLUFRbUU1S01DeUJseGNRQ1FSUHc2WlpkYURKNDhoNGtKQVhYTzlJWncifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ0aW55Y2hlbi10b2tlbi02dzk1NiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ0aW55Y2hlbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImZjMDFhYzE0LTgyZDMtNGZkOS04NmQ0LTUwYWRjYTA0NDE3NSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTp0aW55Y2hlbiJ9.mdda3Xmy7NdAjLpBDdD8Qx-mf9bCIAB2NkVT7VYfRl0dhh4CuJHzgOoPxuqQXPOayU2wiUd2xkwl3yeSrV5DlUW2WPfgKmTVa3_8YeSePjQQb4UlUy1KSnLXUTAB9N294w_JwB2O37ElYsZKgml_BLZNiS2RjcfmvAUd8JPCLizzruZWOtB7u5Q_DbE0nMkC5IPKy8z0YOVgb_6-8Lw3fMFtUglB5gkyuzbCrxy8IE0xSqbBd3im_C43ZCRZf3FQ6TuoNro_ehf1jqC7cQD1uDlex-nWGloufI7MBWedjZDh57fENdVaAwbqoBLMb5cGQ0TwdJqdtrfQ7ZnOHIHgWg
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# 

[root@k8s61 kubernetes-dashboard2.0.0-deploy]# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.244.0.1   <none>        443/TCP   91m
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# kubectl get svc -A
NAMESPACE     NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
default       kubernetes                  ClusterIP   10.244.0.1       <none>        443/TCP                  91m
kube-system   dashboard-metrics-scraper   ClusterIP   10.244.160.136   <none>        8000/TCP                 2m38s
kube-system   kube-dns                    ClusterIP   10.244.0.10      <none>        53/UDP,53/TCP,9153/TCP   91m
kube-system   kubernetes-dashboard        NodePort    10.244.107.60    <none>        443:30443/TCP            2m57s
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# kubectl get pods -A
NAMESPACE     NAME                                        READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-dc4469c7f-tgnmm     1/1     Running   0          30m
kube-system   calico-node-96s69                           1/1     Running   0          15m
kube-system   calico-node-lf24x                           1/1     Running   0          30m
kube-system   calico-node-stx74                           1/1     Running   0          15m
kube-system   coredns-7ff77c879f-gkvb8                    1/1     Running   0          91m
kube-system   coredns-7ff77c879f-vvpn4                    1/1     Running   0          91m
kube-system   dashboard-metrics-scraper-d5698f9b8-sl6rv   1/1     Running   0          2m55s
kube-system   etcd-k8s61                                  1/1     Running   3          91m
kube-system   kube-apiserver-k8s61                        1/1     Running   3          91m
kube-system   kube-controller-manager-k8s61               1/1     Running   3          91m
kube-system   kube-proxy-5gzww                            1/1     Running   3          91m
kube-system   kube-proxy-6xr7b                            1/1     Running   0          15m
kube-system   kube-proxy-b55sw                            1/1     Running   0          15m
kube-system   kube-scheduler-k8s61                        1/1     Running   3          91m
kube-system   kubernetes-dashboard-745974f875-c66sf       1/1     Running   0          3m14s
[root@k8s61 kubernetes-dashboard2.0.0-deploy]# 

[root@k8s61 k8s]# ll
total 316
-rw-r--r-- 1 root root  20678 Apr 12 01:18 calico.yaml
-rw-r--r-- 1 root root 291646 Apr 12 02:05 k8s_install.log
-rw-r--r-- 1 root root    759 Apr 12 00:14 kubeadm.conf
drwxr-xr-x 2 root root    184 Apr 12 01:47 kubernetes-dashboard
drwxr-xr-x 2 root root    114 Apr 12 02:05 kubernetes-metrics-server
[root@k8s61 k8s]# cd kubernetes-metrics-server/
[root@k8s61 kubernetes-metrics-server]# ll
total 16
-rw-r--r-- 1 root root  307 Apr 12 02:05 metrics-api-service.yaml
-rw-r--r-- 1 root root 1844 Apr 12 02:05 metrics-rbac.yaml
-rw-r--r-- 1 root root 1811 Apr 12 02:05 metrics-server-deploy.yaml
-rw-r--r-- 1 root root  202 Apr 12 02:05 README.md
[root@k8s61 kubernetes-metrics-server]# vim metrics-rbac.yaml 
[root@k8s61 kubernetes-metrics-server]# kubectl apply -f metrics-rbac.yaml 
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
[root@k8s61 kubernetes-metrics-server]# kubectl apply -f me^C
[root@k8s61 kubernetes-metrics-server]# vim metrics-api-service.yaml 
[root@k8s61 kubernetes-metrics-server]# kubectl apply -f metrics-api-service.yaml 
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
[root@k8s61 kubernetes-metrics-server]# vim metrics-server-deploy.yaml 
[root@k8s61 kubernetes-metrics-server]# kubectl apply -f metrics-server-deploy.yaml 
service/metrics-server created
deployment.apps/metrics-server created
[root@k8s61 kubernetes-metrics-server]# kubectl get pods -A -o wide
NAMESPACE     NAME                                        READY   STATUS              RESTARTS   AGE    IP               NODE    NOMINATED NODE   READINESS GATES
kube-system   calico-kube-controllers-dc4469c7f-tgnmm     1/1     Running             0          52m    10.244.243.193   k8s61   <none>           <none>
kube-system   calico-node-96s69                           1/1     Running             0          37m    192.168.122.62   k8s62   <none>           <none>
kube-system   calico-node-lf24x                           1/1     Running             0          52m    192.168.122.61   k8s61   <none>           <none>
kube-system   calico-node-stx74                           1/1     Running             0          37m    192.168.122.63   k8s63   <none>           <none>
kube-system   coredns-7ff77c879f-gkvb8                    1/1     Running             0          113m   10.244.243.195   k8s61   <none>           <none>
kube-system   coredns-7ff77c879f-vvpn4                    1/1     Running             0          113m   10.244.243.194   k8s61   <none>           <none>
kube-system   dashboard-metrics-scraper-d5698f9b8-sl6rv   1/1     Running             0          24m    10.244.127.1     k8s62   <none>           <none>
kube-system   etcd-k8s61                                  1/1     Running             3          113m   192.168.122.61   k8s61   <none>           <none>
kube-system   kube-apiserver-k8s61                        1/1     Running             3          113m   192.168.122.61   k8s61   <none>           <none>
kube-system   kube-controller-manager-k8s61               1/1     Running             3          113m   192.168.122.61   k8s61   <none>           <none>
kube-system   kube-proxy-5gzww                            1/1     Running             3          113m   192.168.122.61   k8s61   <none>           <none>
kube-system   kube-proxy-6xr7b                            1/1     Running             0          37m    192.168.122.63   k8s63   <none>           <none>
kube-system   kube-proxy-b55sw                            1/1     Running             0          37m    192.168.122.62   k8s62   <none>           <none>
kube-system   kube-scheduler-k8s61                        1/1     Running             3          113m   192.168.122.61   k8s61   <none>           <none>
kube-system   kubernetes-dashboard-745974f875-c66sf       1/1     Running             0          25m    10.244.138.129   k8s63   <none>           <none>
kube-system   metrics-server-59f947bc97-rmjlx             0/1     ContainerCreating   0          10s    192.168.122.63   k8s63   <none>           <none>
[root@k8s61 kubernetes-metrics-server]# watch kubectl get pods -A -o wide
[root@k8s61 kubernetes-metrics-server]# kubectl top 
node  pod   
[root@k8s61 kubernetes-metrics-server]# kubectl top pod 
No resources found in default namespace.
[root@k8s61 kubernetes-metrics-server]# kubectl top pod -n kube-system 
NAME                                        CPU(cores)   MEMORY(bytes)   
calico-kube-controllers-dc4469c7f-tgnmm     2m           9Mi             
calico-node-96s69                           41m          29Mi            
calico-node-lf24x                           40m          29Mi            
calico-node-stx74                           41m          29Mi            
coredns-7ff77c879f-gkvb8                    4m           12Mi            
coredns-7ff77c879f-vvpn4                    3m           12Mi            
dashboard-metrics-scraper-d5698f9b8-sl6rv   1m           10Mi            
etcd-k8s61                                  33m          63Mi            
kube-apiserver-k8s61                        70m          413Mi           
kube-controller-manager-k8s61               26m          48Mi            
kube-proxy-5gzww                            1m           18Mi            
kube-proxy-6xr7b                            6m           15Mi            
kube-proxy-b55sw                            1m           13Mi            
kube-scheduler-k8s61                        5m           19Mi            
kubernetes-dashboard-745974f875-c66sf       64m          46Mi            
metrics-server-59f947bc97-rmjlx             1m           10Mi            
[root@k8s61 kubernetes-metrics-server]# kubectl top node 
NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s61   288m         7%     1080Mi          13%       
k8s62   103m         2%     559Mi           7%        
k8s63   240m         6%     650Mi           8%        
[root@k8s61 kubernetes-metrics-server]# 